{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "P6y7KQgQGLJj",
        "xLLw6PipG5e0",
        "HVGp23KkpERT",
        "PaeMkL81u3c_",
        "43pW8aYm125B",
        "sBZiuz6M93QW",
        "_6shgha0ASFY",
        "eNcXebwYA-qR",
        "R3_x2linBwZh",
        "oN9Nql3DCcXw",
        "Dgecr7YTDKmP",
        "KiIn5_4uED20",
        "XrMIUcKhMtev",
        "wJgxpdrWmndk",
        "X2yf1A-ZswIM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1° - Convolution of hyperspectral data**"
      ],
      "metadata": {
        "id": "P6y7KQgQGLJj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91fFrPQ2D5aX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Setting the working directory\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/validation_CS/protocol_DB_validation\")\n",
        "\n",
        "# Reading the CSV file\n",
        "DB_raw = pd.read_csv(\"DB_validation_raw_clustered.csv\", encoding='ISO-8859-1')\n",
        "\n",
        "hyper_data = DB_raw\n",
        "\n",
        "# Select the ID column number and the numbers where the hyperspectral bands begin and end\n",
        "hyper_data = hyper_data.iloc[:, [0] + list(range(16, 2167))]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(hyper_data)"
      ],
      "metadata": {
        "id": "RmE0LkB0Ea5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################################\n",
        "#  At this stage of the script, data from hyperspectral bands    #\n",
        "#  to multispectral bands is convolved.                          #\n",
        "#  Pay attention to the script comments at this stage            #\n",
        "##################################################################\n",
        "\n",
        "\n",
        "# Imports\n",
        "# Se estiver executando o script na máquina pessoal, insira o diretório da linguagem R instalada na máquina\n",
        "os.environ['R_HOME'] = 'C:/Program Files/R/R-4.3.2'\n",
        "import rpy2.robjects as robjects\n",
        "from rpy2.robjects.packages import importr\n",
        "\n",
        "# Saving Dataframes to CSV\n",
        "\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/validation_CS/protocol_DB_validation\")\n",
        "hyper_data.to_csv(\"hyper_data.csv\", index=False)\n",
        "\n",
        "# R Script\n",
        "r_script = \"\"\"\n",
        "\n",
        "# Reading CSV files\n",
        "\n",
        "# Ajuste do limite de memória no início do script\n",
        "#memory.limit(size=20000000)\n",
        "\n",
        "setwd(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/validation_CS/protocol_DB_validation\")\n",
        "hyper_data <- read.csv(\"hyper_data.csv\", header = TRUE, sep = \",\")\n",
        "\n",
        "rownames(hyper_data) <- hyper_data[, 1]\n",
        "hyper_data1 <- hyper_data\n",
        "hyper_data1 <- hyper_data1[, -1]\n",
        "ID_Unico <- hyper_data[, 1]\n",
        "colnames(hyper_data1) <- seq(from = 350, to = 2500, by = 1)\n",
        "\n",
        "\n",
        "##############################################################################################\n",
        "## R: Directory path to the required hsdar package libraries##                               #\n",
        "#                                                                                            #\n",
        ".libPaths(\"G:/OneDrive/Documentos/Doutorado/protocol_database/libraires\")                    #\n",
        "#                                                                                            #\n",
        "##############################################################################################\n",
        "\n",
        "# Load the hsdar package\n",
        "library(hsdar)\n",
        "\n",
        "\n",
        "# Create a matrix of the data\n",
        "hyper_data.matrix <- as.matrix(hyper_data1)\n",
        "\n",
        "# Wavelengths for hyperspectral data\n",
        "wave.hyper_data <- seq(from = 350, to = 2500, by = 1)\n",
        "\n",
        "# Create speclib of the data\n",
        "hyper_data.speclib <- speclib(hyper_data.matrix, wave.hyper_data)\n",
        "\n",
        "# Get characteristics of the sensor\n",
        "get.sensor.characteristics(\"Sentinel2a\", response_function = TRUE)\n",
        "\n",
        "\n",
        "############################################################################################\n",
        "#The following command will obtain the channel wavelength of deployed\n",
        "#satellite sensors (multispectral).\n",
        "#Below is a list of sensor names available for operation.\n",
        "#Copy and paste the name into the command:\n",
        "\n",
        "#\"Landsat4\"\n",
        "#\"Landsat5\"\n",
        "#\"Landsat7\"\n",
        "#\"Landsat8\"\n",
        "#\"Sentinel2a\"\n",
        "#\"Sentinel2b\"\n",
        "#\"RapidEye\n",
        "#\"WorldView2-8\"\n",
        "#\"Quickbird\"\n",
        "#\"WorldView2-4\"\n",
        "\n",
        "# Perform spectral resampling\n",
        "multi_data.sentinel2.1 <- spectralResampling(hyper_data.speclib, \"Sentinel2a\",\n",
        "                                            response_function = TRUE)\n",
        "\n",
        "############################################################################################\n",
        "\n",
        "# Plot a graph\n",
        "plot(multi_data.sentinel2.1[1])\n",
        "\n",
        "# Summary statistics\n",
        "summary(multi_data.sentinel2.1)\n",
        "\n",
        "\n",
        "# Round the values before transforming into a dataframe\n",
        "resampled_spectra <- as.data.frame(multi_data.sentinel2.1)\n",
        "\n",
        "#resampled_spectra <- round(as.data.frame(multi_data.sentinel2.1))\n",
        "\n",
        "# Add IDs back\n",
        "resampled_spectra <- cbind(ID_Unico, resampled_spectra)\n",
        "\n",
        "# Write the dataframe to a CSV file\n",
        "setwd(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/validation_CS/protocol_DB_validation\")\n",
        "write.csv(resampled_spectra, \"resampled_spectra.csv\", row.names = FALSE)\n",
        "\n",
        "\"\"\"\n",
        "# Executing the R script\n",
        "robjects.r(r_script)"
      ],
      "metadata": {
        "id": "Qerx7jx2Epo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the working directory\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/validation_CS/protocol_DB_validation\")\n",
        "# Reading the CSV file\n",
        "\n",
        "resampled_spectra = pd.read_csv(\"resampled_spectra.csv\")\n",
        "\n",
        "print(resampled_spectra)"
      ],
      "metadata": {
        "id": "BQudsF_1E9PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data handling for Landsat 8 spectral resolution**"
      ],
      "metadata": {
        "id": "QMV7jy3cq7PL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Landsat 8 spectral resolution:\n",
        "\n",
        "# B1.........Costal...........430-450 (nm)\n",
        "# B2.........Blue.............450-510 (nm)\n",
        "# B3.........Green............530-590 (nm)\n",
        "# B4.........Red..............640-670 (nm)\n",
        "# B5.........NIR..............850-880 (nm)\n",
        "# B9.........Cirrus.........1360-1380 (nm)\n",
        "# B6.........SWIR1..........1570-1650 (nm)\n",
        "# B7.........SWIR2..........2110-2290 (nm)\n",
        "\n",
        "\n",
        "# Create a dictionary of correspondence between original and new names\n",
        "corresponding_name = {\n",
        "    \"ID_Unico\": \"ID_Unico\",\n",
        "    \"Costal\": \"Costal_Landsat8_resampled\",\n",
        "    \"Blue\": \"Blue_Landsat8_resampled\",\n",
        "    \"Green\": \"Green_Landsat8_resampled\",\n",
        "    \"Red\": \"Red_Landsat8_resampled\",\n",
        "    \"NIR\": \"NIR_Landsat8_resampled\",\n",
        "    \"Cirrus\": \"Cirrus_Landsat8_resampled\",\n",
        "    \"SWIR1\": \"SWIR1_Landsat8_resampled\",\n",
        "    \"SWIR2\": \"SWIR2_Landsat8_resampled\"\n",
        "}\n",
        "\n",
        "# Rename the columns of the resampled_spectra dataframe\n",
        "resampled_spectra.columns = [corresponding_name[col] for col in resampled_spectra.columns]\n",
        "\n",
        "# Merge the dataframes\n",
        "DB_raw_resampled_spectra = pd.merge(DB_raw, resampled_spectra, on=\"ID_Unico\")"
      ],
      "metadata": {
        "id": "Rw94Jn70i2pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(DB_raw_resampled_spectra)"
      ],
      "metadata": {
        "id": "kB_Bk3zmrQdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving Dataframes to CSV\n",
        "DB_raw_resampled_spectra.to_csv(\"DB_raw_resampled_spectra.csv\", index=False)"
      ],
      "metadata": {
        "id": "XPYQGRbxrRNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data handling for Sentinel-2A spectral resolution**"
      ],
      "metadata": {
        "id": "Ihy671sqrV2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentinel-2A spectral resolution:\n",
        "# B1.........Aerossol.........430-450 (nm)\n",
        "# B2.........Blue.............458-523 (nm)\n",
        "# B3.........Green............543-578 (nm)\n",
        "# B4.........Red..............650-680 (nm)\n",
        "# B5.........Red Edge 1.......698-713 (nm)\n",
        "# B6.........Red Edge 2.......733-748 (nm)\n",
        "# B7.........Red Edge 3.......773-793 (nm)\n",
        "# B8.........NIR..............785-899 (nm)\n",
        "# B8A........Red Edge 4.......855-875 (nm)\n",
        "# B9.........Water Vapor......930-950 (nm)\n",
        "# B10........Cirrus...........1360-1390 (nm)\n",
        "# B11........SWIR 1...........1565-1655 (nm)\n",
        "# B12........SWIR 2...........2100-2280 (nm)\n",
        "\n",
        "# Create a dictionary of correspondence between original and new names\n",
        "corresponding_name = {\n",
        "    \"ID_Unico\": \"ID_Unico\",\n",
        "    \"SR_AV_B1\": \"Aerosol_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B2\": \"Blue_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B3\": \"Green_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B4\": \"Red_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B5\": \"Red_Edge_1_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B6\": \"Red_Edge_2_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B7\": \"Red_Edge_3_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B8\": \"NIR_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B8A\": \"Red_Edge_4_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B9\": \"Water_Vapor_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B10\": \"Cirrus_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B11\": \"SWIR_1_Sentinel2A_resampled\",\n",
        "    \"SR_AV_B12\": \"SWIR_2_Sentinel2A_resampled\"\n",
        "}\n",
        "\n",
        "# Rename the columns of the resampled_spectra dataframe\n",
        "resampled_spectra.columns = [corresponding_name[col] for col in resampled_spectra.columns]\n",
        "\n",
        "# Merge the dataframes\n",
        "DB_raw_resampled_spectra = pd.merge(DB_raw, resampled_spectra, on=\"ID_Unico\")"
      ],
      "metadata": {
        "id": "4DD7bYDcFMd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(DB_raw_resampled_spectra)"
      ],
      "metadata": {
        "id": "kZADYvObFTxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving Dataframes to CSV\n",
        "DB_raw_resampled_spectra.to_csv(\"DB_validation_raw_resampled_spectra.csv\", index=False)"
      ],
      "metadata": {
        "id": "ckjhbvTkFU2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2° - Filter by equations**"
      ],
      "metadata": {
        "id": "xLLw6PipG5e0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 2.2 - Filter equation tendency**\n",
        "\n",
        "The tendency equation is a rule that expresses the spectral signature of the soil. The spectral bands Blue, Green, Red, and NIR must necessarily follow the order of increasing values. This order is: Blue < Green < Red < NIR."
      ],
      "metadata": {
        "id": "HVGp23KkpERT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Setting the working directory\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering\")\n",
        "\n",
        "# Reading the CSV file\n",
        "DB_raw = pd.read_csv(\"DB_raw_resampled_spectra.csv\", sep=',')\n",
        "\n",
        "# Exibindo o DataFrame incial\n",
        "print(DB_raw)"
      ],
      "metadata": {
        "id": "CXacz4QdUM0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Creating the new DataFrame\n",
        "results_equation_tendency = pd.DataFrame()\n",
        "\n",
        "# Adding the 'ID_Unico' column to the new DataFrame\n",
        "results_equation_tendency['ID_Unico'] = DB_raw['ID_Unico']\n",
        "\n",
        "############################################################################\n",
        "# Change the names of the spectral bands, Blue, Green, Red, and NIR, if\n",
        "# the names of these respective bands in the columns of your dataset\n",
        "# are different.\n",
        "\n",
        "# Adding the 'outlier_tendency' column based on the condition\n",
        "condition = (\n",
        "    (DB_raw['Blue_SySI_Sentinel'] < DB_raw['Green_SySI_Sentinel']) &\n",
        "    (DB_raw['Green_SySI_Sentinel'] < DB_raw['Red_SySI_Sentinel']) &\n",
        "    (DB_raw['Red_SySI_Sentinel'] < DB_raw['NIR_SySI_Sentinel'])\n",
        ")\n",
        "############################################################################\n",
        "\n",
        "results_equation_tendency['outlier_tendency'] = ~condition\n",
        "\n",
        "# Displaying the new DataFrame\n",
        "#print(results_equation_tendency)\n",
        "\n",
        "# Adding the 'outlier_tendency' column to the original DataFrame using merge\n",
        "DB_tendency = pd.merge(DB_raw, results_equation_tendency[['ID_Unico', 'outlier_tendency']], on='ID_Unico')\n",
        "\n",
        "# Storing the 'outlier_tendency' column in a temporary variable\n",
        "outlier_tendency_col = DB_tendency['outlier_tendency']\n",
        "\n",
        "# Removing the 'outlier_tendency' column from the DataFrame\n",
        "DB_tendency = DB_tendency.drop('outlier_tendency', axis=1)\n",
        "\n",
        "# Inserting the 'outlier_tendency' column at the desired position (position 2)\n",
        "DB_tendency.insert(2, 'outlier_tendency', outlier_tendency_col)\n",
        "\n",
        "# DataFrame for True values in 'outlier_tendency'\n",
        "outliers_samples_tendency = DB_tendency[DB_tendency['outlier_tendency'] == True]\n",
        "\n",
        "# DataFrame for False values in 'outlier_tendency'\n",
        "DB_filter_tendency = DB_tendency[DB_tendency['outlier_tendency'] == False]\n",
        "\n",
        "# Displaying the final DataFrame\n",
        "print(DB_filter_tendency)\n"
      ],
      "metadata": {
        "id": "bMyBdXylo-hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the number of samples in the initial dataframe,\n",
        "# outliers tendency dataframe, and filtered dataframe\n",
        "\n",
        "# Print the number of samples in DB_filter_lignin\n",
        "print(\"Number of samples in DB_raw:\", DB_raw.shape[0])\n",
        "\n",
        "# Print the number of samples in outliers_samples_tendency\n",
        "print(\"Number of samples in outliers_samples_tendency:\", outliers_samples_tendency.shape[0])\n",
        "\n",
        "# Print the number of samples in DB_filter_tendency\n",
        "print(\"Number of samples in DB_filter_tendency:\", DB_filter_tendency.shape[0])"
      ],
      "metadata": {
        "id": "6s_hb_3qpKEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_spectra(data, plot_name: str = 'spectra_plot'):\n",
        "    # Dicionário para mapear nomes de colunas para rótulos mais curtos\n",
        "    label_map = {\n",
        "        'Blue_SySI_Sentinel': 'Blue (458-523 nm)',\n",
        "        'Green_SySI_Sentinel': 'Green (543-578 nm)',\n",
        "        'Red_SySI_Sentinel': 'Red (650-680 nm)',\n",
        "        'Red_Edge_1_10m_SySI_Sentinel': 'Red Edge 1 (698-713 nm)',\n",
        "        'Red_Edge_2_10m_SySI_Sentinel': 'Red Edge 2 (733-748 nm)',\n",
        "        'Red_Edge_3_10m_SySI_Sentinel': 'Red Edge 3 (773-793 nm)',\n",
        "        'NIR_SySI_Sentinel': 'NIR (785-899 nm)',\n",
        "        'Red_Edge_4_10m_SySI_Sentinel': 'Red Edge 4 (855-875 nm)',\n",
        "        'SWIR_1_10m_SySI_Sentinel': 'SWIR 1 (1565-1655 nm)',\n",
        "        'SWIR_2_10m_SySI_Sentinel': 'SWIR 2 (2100-2280 nm)'\n",
        "    }\n",
        "\n",
        "    # Especificar as colunas espectrais\n",
        "    spectral_columns = list(label_map.keys())\n",
        "\n",
        "    # Filtrar colunas que existem no DataFrame\n",
        "    existing_columns = [col for col in spectral_columns if col in data.columns]\n",
        "    # Obter rótulos curtos\n",
        "    short_labels = [label_map[col] for col in existing_columns]\n",
        "\n",
        "    plt.figure(figsize=(12, 7))\n",
        "\n",
        "    # Iterando sobre cada linha nos dados\n",
        "    for index, row in data.iterrows():\n",
        "        # Plotar cada linha com base nas colunas espectrais\n",
        "        plt.plot(short_labels, row[existing_columns], label=f'Outlier {index}')\n",
        "\n",
        "    plt.xlabel('Spectral Band')\n",
        "    plt.ylabel('Reflectance Factor')\n",
        "    plt.title(f'Spectral Signatures of {plot_name}')\n",
        "\n",
        "    # Rotacionar os labels do eixo x para evitar sobreposição\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Definir limites para o eixo y\n",
        "    plt.ylim(0, 0.8)  # Ajuste conforme necessário\n",
        "\n",
        "    plt.grid(False)\n",
        "\n",
        "    # Salvar o gráfico com alta resolução\n",
        "    output_path = \"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/DB_Raw.png\"\n",
        "    dpi_value = 1200\n",
        "    plt.savefig(output_path, dpi=dpi_value, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "rJvnGll-ELAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_spectra(DB_filter_tendency, 'DB Raw')"
      ],
      "metadata": {
        "id": "J-jTGhR8Errq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DB_filter_step_2 DataFrame as a CSV file\n",
        "# Setting the working directory\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering\")\n",
        "\n",
        "DB_filter_tendency.to_csv(\"DB_filter_step_2_SySI_data.csv\", index=False)\n",
        "outliers_samples_tendency.to_csv(\"outliers_samples_tendency.csv\", index=False)"
      ],
      "metadata": {
        "id": "euxQXewvpNQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3° - Filter by PCA and distance Mahalanobis**"
      ],
      "metadata": {
        "id": "PaeMkL81u3c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages required\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.stats import chi2\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import os"
      ],
      "metadata": {
        "id": "Y5seQqAzvA23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the working directory\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering\")\n",
        "\n",
        "# Importing soil spectra\n",
        "DB_filter_step_2 = pd.read_csv(\"DB_filter_step_2_SySI_data.csv\", low_memory=False)\n",
        "DB_filter_step_2 = DB_filter_step_2.set_index('ID_Unico') #Assigning row names from ID column\n",
        "\n",
        "print(DB_filter_step_2)"
      ],
      "metadata": {
        "id": "i89-iUVx66fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### spectra selection ###\n",
        "\n",
        "# Specify the index of the desired columns\n",
        "col_index_min = 12  # Index of the initial column (adjust as needed)\n",
        "col_index_max = 21 # Index of the final column (adjust as needed)\n",
        "\n",
        "# Select columns based on index\n",
        "data = DB_filter_step_2.iloc[:, col_index_min:col_index_max + 1]  # soil spectra (original)\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "id": "XXbTWEGu-c1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### standardized data ###\n",
        "\n",
        "data_selected = data.apply(pd.to_numeric, errors='coerce')\n",
        "data_selected.fillna(data_selected.min(), inplace=True)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = MinMaxScaler()\n",
        "data_standardized = pd.DataFrame(scaler.fit_transform(data), index=data.index)\n",
        "\n",
        "# Convert the standardized data back to a DataFrame\n",
        "data_standardized_df = pd.DataFrame(data_standardized)"
      ],
      "metadata": {
        "id": "2KD1PVn17C1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_standardized)"
      ],
      "metadata": {
        "id": "d12pNLhTaNAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IDENTIY OUTLIERS FUNCTION\n",
        "def identify_outliers(x, n_pc_comp=0):\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    x_scaled = scaler.fit_transform(x)\n",
        "\n",
        "    # Handle NaN values (replace with mean of each column)\n",
        "    x_scaled = np.nan_to_num(x_scaled, nan=np.nanmean(x_scaled, axis=0))\n",
        "\n",
        "    # Perform PCA\n",
        "    pca = PCA()\n",
        "    pr_scores = pca.fit_transform(x_scaled)\n",
        "    explained_variance = pca.explained_variance_ratio_\n",
        "    cpr = np.cumsum(explained_variance)\n",
        "\n",
        "    # Determine number of principal components\n",
        "    if n_pc_comp == 0:\n",
        "        n_pc_comp = np.min(np.where(cpr > 0.999)[0] + 1)\n",
        "    elif 0 < n_pc_comp < 1:\n",
        "        n_pc_comp = np.min(np.where(cpr > n_pc_comp)[0] + 1)\n",
        "\n",
        "    # Calculate mean and covariance\n",
        "    mean_pcaA = np.mean(pr_scores[:, :n_pc_comp], axis=0)\n",
        "    cov_pcaA = np.cov(pr_scores[:, :n_pc_comp], rowvar=False)\n",
        "\n",
        "    # Calculate Mahalanobis distance\n",
        "    chiMat = np.zeros((pr_scores.shape[0], 3))\n",
        "    chiMat[:, 0] = np.array([np.dot(np.dot((pr_scores[i, :n_pc_comp] - mean_pcaA), np.linalg.inv(cov_pcaA)), (pr_scores[i, :n_pc_comp] - mean_pcaA).T) for i in range(pr_scores.shape[0])])\n",
        "\n",
        "    # Fit Chi-Squared distribution\n",
        "    chiMat[:, 1] = chi2.cdf(chiMat[:, 0], df=n_pc_comp)\n",
        "\n",
        "    # Determine critical value\n",
        "    if n_pc_comp <= 10:\n",
        "        pcrit = 1 - ((0.24 - 0.003 * n_pc_comp) / np.sqrt(pr_scores.shape[0]))\n",
        "    else:\n",
        "        pcrit = 1 - ((0.25 - 0.0018 * n_pc_comp) / np.sqrt(pr_scores.shape[0]))\n",
        "    pcrit = 0.999\n",
        "    #pcrit = 0.995\n",
        "\n",
        "    # Identify outliers\n",
        "    chiMat[:, 2] = np.where(chiMat[:, 1] >= pcrit, 0, 1)\n",
        "\n",
        "    # Return the unique IDs of the outliers\n",
        "    outliers_indices = np.where(chiMat[:, 2] == 0)[0]\n",
        "    outliers_ids = x.index[outliers_indices]\n",
        "\n",
        "    return outliers_ids\n"
      ],
      "metadata": {
        "id": "UnU884tV7nwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the function and passing the original DataFrame\n",
        "outliers_ids = identify_outliers(data, n_pc_comp=0.995)\n",
        "\n",
        "# Exibir os IDs únicos dos outliers\n",
        "print(outliers_ids)"
      ],
      "metadata": {
        "id": "KCgjqr1F7tOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_data_step_3 = data.drop(index=outliers_ids)\n",
        "outliers_data_step_3 = data.loc[outliers_ids]\n",
        "\n",
        "print(outliers_data_step_3)"
      ],
      "metadata": {
        "id": "QlxIPBKN8uXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "import os\n",
        "\n",
        "def plot_spectra(data, plot_name: str = 'spectra_plot', save_path=None, dpi: int = 1000):\n",
        "    # Dicionário para mapear nomes de colunas para rótulos mais curtos\n",
        "    label_map = {\n",
        "        'Blue_SySI_Sentinel': 'Blue (458-523 nm)',\n",
        "        'Green_SySI_Sentinel': 'Green (543-578 nm)',\n",
        "        'Red_SySI_Sentinel': 'Red (650-680 nm)',\n",
        "        'Red_Edge_1_10m_SySI_Sentinel': 'Red Edge 1 (698-713 nm)',\n",
        "        'Red_Edge_2_10m_SySI_Sentinel': 'Red Edge 2 (733-748 nm)',\n",
        "        'Red_Edge_3_10m_SySI_Sentinel': 'Red Edge 3 (773-793 nm)',\n",
        "        'NIR_SySI_Sentinel': 'NIR (785-899 nm)',\n",
        "        'Red_Edge_4_10m_SySI_Sentinel': 'Red Edge 4 (855-875 nm)',\n",
        "        'SWIR_1_10m_SySI_Sentinel': 'SWIR 1 (1565-1655 nm)',\n",
        "        'SWIR_2_10m_SySI_Sentinel': 'SWIR 2 (2100-2280 nm)'\n",
        "    }\n",
        "    # Especificar as colunas espectrais\n",
        "    spectral_columns = list(label_map.keys())\n",
        "    # Filtrar colunas que existem no DataFrame\n",
        "    existing_columns = [col for col in spectral_columns if col in data.columns]\n",
        "    # Obter rótulos curtos\n",
        "    short_labels = [label_map[col] for col in existing_columns]\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Iterando sobre cada linha nos dados\n",
        "    for index, row in data.iterrows():\n",
        "        # Plotar cada linha com base nas colunas espectrais\n",
        "        plt.plot(short_labels, row[existing_columns], label=f'Outlier {index}')\n",
        "\n",
        "    plt.xlabel('Spectral Band')\n",
        "    plt.ylabel('Reflectance Factor')\n",
        "    plt.title(f'Spectral Signatures of {plot_name}')\n",
        "\n",
        "    # Rotacionar os labels do eixo x para evitar sobreposição\n",
        "    plt.xticks(rotation=45)\n",
        "    # Definir limites para o eixo y\n",
        "    plt.ylim(0, 0.8)  # Ajuste conforme necessário\n",
        "    plt.grid(False)\n",
        "    plt.savefig(f'{plot_name}.png', dpi=dpi, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_3d_pca_with_outliers(data_standardized, outliers_ids, save_path=None, dpi: int = 1000):\n",
        "    \"\"\"\n",
        "    Plots a 3D PCA plot highlighting outliers.\n",
        "\n",
        "    Parameters:\n",
        "    - data_standardized: The standardized data (samples x features).\n",
        "    - outliers_ids: Indices of the outliers in the dataset.\n",
        "    - save_path: (Optional) Path to save the plot. If None, the plot is not saved.\n",
        "    \"\"\"\n",
        "    # Handle NaN values before applying PCA\n",
        "    data_standardized = data_standardized.fillna(data_standardized.mean()) # Fill NaN with column means\n",
        "\n",
        "    # Perform PCA\n",
        "    pca = PCA(n_components=3)\n",
        "    pr_scores = pd.DataFrame(pca.fit_transform(data_standardized), index=data_standardized.index)\n",
        "\n",
        "    # Extracting the outliers and non-outliers\n",
        "    outliers = pr_scores.loc[outliers_ids]\n",
        "    non_outliers = pr_scores.drop(index=outliers_ids)\n",
        "\n",
        "    # Convert outliers and non_outliers to numpy arrays for plotting\n",
        "    outliers_array = outliers.to_numpy()\n",
        "    non_outliers_array = non_outliers.to_numpy()\n",
        "\n",
        "    # Create 3D plot\n",
        "    fig = plt.figure(figsize=(14, 16))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # Plot outliers\n",
        "    ax.scatter(outliers_array[:, 0], outliers_array[:, 1], outliers_array[:, 2], c='red', marker='x', s=50, label=\"Outliers\")\n",
        "\n",
        "    # Plot non-outliers\n",
        "    ax.scatter(non_outliers_array[:, 0], non_outliers_array[:, 1], non_outliers_array[:, 2], c='gray', marker='o', alpha=0.6, label=\"Non-outliers\")\n",
        "\n",
        "    # Label axes with explained variance ratios\n",
        "    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0] * 100:.2f}%)')\n",
        "    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1] * 100:.2f}%)')\n",
        "    ax.set_zlabel(f'PC3 ({pca.explained_variance_ratio_[2] * 100:.2f}%)')\n",
        "\n",
        "    # Add legend\n",
        "    ax.legend()\n",
        "\n",
        "    # Set the plot title\n",
        "    plt.title('3D PCA plot with Outliers')\n",
        "\n",
        "    # Save the figure if a path is provided\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=dpi, bbox_inches='tight')\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "IL-PFaWf80Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plots Visualization\n",
        "\n",
        "# Setting the working directory\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/Step_3_SySI_data\")\n",
        "\n",
        "plot_spectra(filtered_data_step_3, 'filtered_data_step_3')\n",
        "plot_spectra(outliers_data_step_3, 'outliers')\n",
        "plot_spectra(data, 'all_data')\n",
        "plot_3d_pca_with_outliers(data, outliers_ids, save_path='3d_pca_outliers')"
      ],
      "metadata": {
        "id": "OZYT5tCW84XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outliers_data_step_3 = outliers_data_step_3.reset_index()\n",
        "\n",
        "IDs_to_filter = outliers_data_step_3['ID_Unico'].tolist()\n",
        "\n",
        "DB_filter_step_3 = DB_filter_step_2.reset_index()\n",
        "\n",
        "# Add a new column \"outliers_step_3\" filled with False\n",
        "DB_filter_step_3.insert(1, \"outliers_step_3\", False)\n",
        "\n",
        "# Update the values to True where ID_Unico is in the list IDs_to_filter\n",
        "DB_filter_step_3.loc[DB_filter_step_3['ID_Unico'].isin(IDs_to_filter), 'outliers_step_3'] = True\n",
        "\n",
        "# Create a new DataFrame with only the rows where outliers_step_3 is True\n",
        "outliers_samples_step_3 = DB_filter_step_3[DB_filter_step_3['outliers_step_3'] == True].copy()\n",
        "\n",
        "# Remove the samples where outliers_step_3 is True\n",
        "DB_filter_step_3 = DB_filter_step_3[DB_filter_step_3['outliers_step_3'] == False].copy()\n",
        "\n",
        "print(outliers_samples_step_3)"
      ],
      "metadata": {
        "id": "TYgANWBE87jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the number of samples in the initial dataframe,\n",
        "# outliers_samples_step_3 dataframe, and filtered dataframe\n",
        "\n",
        "# Print the number of samples in DB_filter_step_2\n",
        "print(\"Number of samples in DB_filter_step_2:\", DB_filter_step_2.shape[0])\n",
        "\n",
        "# Print the number of samples in outliers_step_3\n",
        "print(\"Number of samples in outliers_samples_step_3:\", outliers_samples_step_3.shape[0])\n",
        "\n",
        "# Print the number of samples in DB_filter_step_3\n",
        "print(\"Number of samples in DB_filter_step_3:\", DB_filter_step_3.shape[0])"
      ],
      "metadata": {
        "id": "RMqQcIGG9NRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting the working directory\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/Step_3_SySI_data\")\n",
        "\n",
        "# Save the outliers_samples_3 DataFrame as a CSV file\n",
        "outliers_samples_step_3.to_csv(\"outliers_samples_step_3.csv\", index=False)\n",
        "\n",
        "# Save the DB_filter_3 DataFrame as a CSV file\n",
        "DB_filter_step_3.to_csv(\"DB_validation_filter_step_3.csv\", index=False)"
      ],
      "metadata": {
        "id": "EjQfz5Yq9kbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visual analysis of the outiliers determined by Step 3°**\n",
        "\n",
        "This sub-step is not mandatory for completing the entire protocol, but we strongly recommend that the spectral signatures of samples identified as outliers be visually analyzed. This analysis allows the pedometrist to understand the nature of the outlier spectrum and why the data might be inconsistent or incorrect. Examples include:\n",
        "\n",
        "- Miscalibration of the Vis-NIR-SWIR sensor, resulting in noise in the spectral signature.\n",
        "- Contamination of the soil sample with other materials.\n",
        "- Improper preparation of the soil sample.\n",
        "- Collection of the spectral data from a target other than the soil sample.\n",
        "\n",
        "By understanding the nature of the error, the pedometrist can intervene at the source, preventing similar errors from occurring in the future.\n",
        "However, it is important to note that the spectral data may be correct. The combination of PCA and Mahalanobis distance may flag an outlier that actually corresponds to a soil with distinct spectral behavior, which is not well-represented in the dataset. Therefore, if you identify a sample whose spectral behavior matches all the correct spectral characteristics of a soil in the \"outliers_samples_step_3\" dataset, we recommend including this sample's data in the \"DB_filter_step_3\" dataset.\n",
        "\n",
        "The following interactive plots of the hyperspectral data from soil samples will be generated for visual analysis."
      ],
      "metadata": {
        "id": "43pW8aYm125B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages required\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px"
      ],
      "metadata": {
        "id": "lmIznPTy2C1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outliers_samples_step_3)"
      ],
      "metadata": {
        "id": "jJSefgi78cag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the indices of the columns you want to select\n",
        "col_index_min = 109  # Index of the first spectral column\n",
        "col_index_max = 2189  # Index of the last spectral column\n",
        "\n",
        "# Select the ID column by name and the spectral columns by indices\n",
        "selected_columns = [outliers_samples_step_3.columns.get_loc('ID_Unico')] + list(range(col_index_min, col_index_max + 1))\n",
        "\n",
        "# Create a new DataFrame with the desired columns\n",
        "DB_graphic = outliers_samples_step_3.iloc[:, selected_columns].copy()\n",
        "\n",
        "print(DB_graphic)"
      ],
      "metadata": {
        "id": "QS4bt6Km8oqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataframe into smaller dataframes based on rows, in this case 50 rows\n",
        "dfs = np.array_split(DB_graphic, np.ceil(len(DB_graphic) / 100))\n",
        "\n",
        "# Create the line chart using Plotly Express for each smaller dataframe\n",
        "for i, df in enumerate(dfs):\n",
        "    # Reshape the dataframe\n",
        "    df_melt = df.melt(id_vars=df.columns[0], value_vars=df.columns[1:])\n",
        "\n",
        "    fig = px.line(df_melt, x='variable', y='value', color=df.columns[0],\n",
        "                  title=f'Original Spectra {i+1}', labels={'variable': 'Wavelength (nm)', 'value': 'Reflectance Factor'})\n",
        "\n",
        "    # Customize the chart layout\n",
        "    fig.update_layout(\n",
        "        xaxis_title='Wavelength (nm)',\n",
        "        yaxis_title='Reflectance Factor',\n",
        "    )\n",
        "\n",
        "    # Build the HTML file name based on the chart\n",
        "    html_file_name = os.path.join('G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/Step_3', f'Original_Spectra_Plotly_{i+1}.html')\n",
        "\n",
        "    # Save the interactive chart to an HTML file\n",
        "    fig.write_html(html_file_name)\n",
        "\n",
        "    # Display the interactive chart\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "KHUNBjnn-_SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4° - Filter by Soil Line**"
      ],
      "metadata": {
        "id": "sBZiuz6M93QW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages required\n",
        "import os\n",
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from plotnine import *\n",
        "from plotnine import ggsave\n",
        "import scipy.stats as stats\n",
        "from plotnine import ggplot"
      ],
      "metadata": {
        "id": "4pHaPhQA-Kkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Soil line on convolved spectra ###\n",
        "\n",
        "# Set the working directory\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering\\Step_3_SySI_data\")\n",
        "\n",
        "# Read the CSV file\n",
        "DB_filter_step_3 = pd.read_csv(\"DB_validation_filter_step_3.csv\")\n",
        "\n",
        "# Assigning to a variable for clarity\n",
        "DB = DB_filter_step_3\n",
        "\n",
        "print(DB_filter_step_3)"
      ],
      "metadata": {
        "id": "vN32n1N9_Pex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cutting line determination ###\n",
        "\n",
        "# Specify the X and Y columns\n",
        "column_x_conv = DB['Red_SySI_Sentinel']\n",
        "column_y_conv = DB['NIR_SySI_Sentinel']\n",
        "\n",
        "# Create a linear regression model\n",
        "X = sm.add_constant(column_x_conv)  # Add the constant for the intercept term\n",
        "model = sm.OLS(column_y_conv, X).fit()\n",
        "\n",
        "# Calculate predicted values\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Calculate the standard deviation of residuals\n",
        "residuals = model.resid\n",
        "std_residuals = np.std(residuals)\n",
        "\n",
        "# Confidence level (e.g., 98%)\n",
        "confidence_level = 0.99\n",
        "\n",
        "# Quantile of the t distribution\n",
        "quantile_t = stats.t.ppf((1 + confidence_level) / 2, df=len(residuals) - 2)\n",
        "\n",
        "# Calculate upper and lower confidence limits\n",
        "upper_limit_NIR_resam = predictions + quantile_t * (std_residuals * 1.25)\n",
        "lower_limit_NIR_resam = predictions - quantile_t * (std_residuals * 1.25)\n",
        "\n",
        "# Regression coefficients\n",
        "intercept, coef_x = model.params\n",
        "\n",
        "# Standard error of coefficients\n",
        "std_error = model.bse\n",
        "\n",
        "# T-value and p-value of coefficients\n",
        "t_value = model.tvalues\n",
        "p_value = model.pvalues\n",
        "\n",
        "# Coefficient of determination (R²)\n",
        "r_squared_convolved = model.rsquared\n",
        "\n",
        "# Fit a linear regression model for the upper limit\n",
        "model_upper_limit = sm.OLS(upper_limit_NIR_resam, X).fit()\n",
        "\n",
        "# Fit a linear regression model for the lower limit\n",
        "model_lower_limit = sm.OLS(lower_limit_NIR_resam, X).fit()\n",
        "\n",
        "# Coefficients of intercept and slope for the upper limit\n",
        "coef_intercept_upper, coef_slope_upper = model_upper_limit.params\n",
        "\n",
        "# Coefficients of intercept and slope for the lower limit\n",
        "coef_intercept_lower, coef_slope_lower = model_lower_limit.params\n",
        "\n",
        "# Display coefficients\n",
        "print(\"Coefficients for Upper Limit:\")\n",
        "print(\"Intercept (β0):\", round(coef_intercept_upper, 4))\n",
        "print(\"Slope (β1):\", round(coef_slope_upper, 4))\n",
        "\n",
        "print(\"\\nCoefficients for Lower Limit:\")\n",
        "print(\"Intercept (β0):\", round(coef_intercept_lower, 4))\n",
        "print(\"Slope (β1):\", round(coef_slope_lower, 4))"
      ],
      "metadata": {
        "id": "mmleqpx4_aGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### plot the Soil line by convolved data ###\n",
        "\n",
        "# Desired width and height of the plot\n",
        "plot_width = 9\n",
        "plot_height = 6\n",
        "\n",
        "soil_line_plot = (\n",
        "    ggplot(DB, aes(x='Red_SySI_Sentinel', y='NIR_SySI_Sentinel')) +\n",
        "    theme_classic() +\n",
        "    geom_point(shape='o', color= 'black', fill= \"#E49247\", alpha=1, size=1.5, stroke=0.2) + # Data points with black contour +  # Data points\n",
        "    geom_smooth(method='lm', se=False, color='#000000', size=1.2) +  # Trend line\n",
        "    geom_line(aes(y=upper_limit_NIR_resam), color='#E41A1C', size=1) +  # Upper confidence interval line\n",
        "    geom_line(aes(y=lower_limit_NIR_resam), color='#E41A1C', size=1) +  # Lower confidence interval line\n",
        "\n",
        "    annotate('text', x=0.11, y=0.58, label=\"β upper limit: {:.3f}\".format(round(coef_intercept_upper, 3)),\n",
        "             family=\"Arial\", size=10)+\n",
        "    annotate('text', x=0.11, y=0.54, label=\"β lower limit: {:.3f}\".format(round(coef_intercept_lower, 3)),\n",
        "             family=\"Arial\", size=10)+\n",
        "    annotate('text', x=0.11, y=0.50, label=\"R² model: {:.3f}\".format(round(r_squared_convolved, 3)),\n",
        "             family=\"Arial\", size=10)+\n",
        "    annotate('text', x=0.11, y=0.46, label=\"α: {:.3f}\".format(round(coef_x, 3)),\n",
        "             family=\"Arial\", size=10)+\n",
        "\n",
        "    labs(x=\"SySI Red band (650-680 nm)\", y=\"SySI NIR band (785-899 nm)\") +\n",
        "    expand_limits(x=0, y=0) +  # Force the plot to show the origin (0,0)\n",
        "    scale_y_continuous(limits=[0, 0.6]) +\n",
        "    scale_x_continuous(limits=[0, 0.6]) +\n",
        "    theme(\n",
        "        axis_title_x=element_text(margin={'t': 10}, family=\"Arial\", size=12, face=\"bold\"),\n",
        "        axis_title_y=element_text(margin={'r': 10}, family=\"Arial\", size=12, face=\"bold\"),\n",
        "        axis_text_y=element_text(family=\"Arial\", size=10),\n",
        "        axis_text_x=element_text(family=\"Arial\", size=10)\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "# Display the plot\n",
        "print(soil_line_plot)\n",
        "\n",
        "dpi_value = 1000\n",
        "\n",
        "# Setting the working directory\n",
        "output_path = \"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/Step_4_SySI_data/soil_line_plot.png\"\n",
        "soil_line_plot.save(output_path, width=plot_width, height=plot_height, dpi=dpi_value)\n"
      ],
      "metadata": {
        "id": "9SRGiz8t_lz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## After Soil Line analysis, filtering the samples in the database ###\n",
        "\n",
        "# Create a subset of samples that are ABOVE the upper_limit_NIR line\n",
        "samples_upper_limit = DB[DB['NIR_SySI_Sentinel'] > upper_limit_NIR_resam]\n",
        "\n",
        "# Create a subset of samples that are BELOW the lower_limit_NIR line\n",
        "samples_lower_limit = DB[DB['NIR_SySI_Sentinel'] < lower_limit_NIR_resam]\n",
        "\n",
        "# Concatenate the dataframes samples_upper_limit and samples_lower_limit\n",
        "outliers_samples = pd.concat([samples_upper_limit, samples_lower_limit])\n",
        "\n",
        "# Merge the DataFrames based on the ID_Unico column and keep only the columns from the DB DataFrame\n",
        "outliers_samples_step_4 = pd.merge(outliers_samples[['ID_Unico']], DB, on='ID_Unico', how='inner')\n",
        "\n",
        "IDs_to_filter = outliers_samples['ID_Unico'].tolist()\n",
        "\n",
        "DB_filter_step_4 = DB_filter_step_3\n",
        "\n",
        "# Add a new column \"outliers_step_4\" filled with False\n",
        "DB_filter_step_4.insert(1, \"outliers_step_4\", False)\n",
        "\n",
        "# Update the values to True where ID_Unico is in the list IDs_to_filter\n",
        "DB_filter_step_4.loc[DB_filter_step_4['ID_Unico'].isin(IDs_to_filter), 'outliers_step_4'] = True\n",
        "\n",
        "# Create a new DataFrame with only the rows where outliers_step_4 is True\n",
        "outliers_samples_step_4 = DB_filter_step_4[DB_filter_step_4['outliers_step_4'] == True].copy()\n",
        "\n",
        "# Delete the samples where outliers_step_3 is True\n",
        "DB_filter_step_4 = DB_filter_step_4[DB_filter_step_4['outliers_step_4'] == False].copy()\n",
        "\n",
        "print(outliers_samples_step_4)"
      ],
      "metadata": {
        "id": "Yp4IfsPU_7yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the number of samples in the initial dataframe of the attribute that was analyzed,\n",
        "# outliers outliers_samples_step_4, and DB_filter_step_4 dataframe\n",
        "\n",
        "# Print the number of samples in DB_filter_step_3\n",
        "print(\"Number of samples in DB_filter_step_3:\", DB_filter_step_3.shape[0])\n",
        "\n",
        "# Print the number of samples in outliers_samples_step_4\n",
        "print(\"Number of samples in outliers_samples_step_4:\", outliers_samples_step_4.shape[0])\n",
        "\n",
        "# Print the number of samples in DB_filter_step_4\n",
        "print(\"Number of samples in DB_filter_step_4:\", DB_filter_step_4.shape[0])"
      ],
      "metadata": {
        "id": "j52kT6UpAAd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DB_filter_step_4 and outliers_samples_step_4 DataFrame as a CSV file\n",
        "# Setting the working directory\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/Step_4_SySI_data\")\n",
        "\n",
        "outliers_samples_step_4.to_csv(\"outliers_samples_step_4_SySI_data.csv\", index=False)\n",
        "\n",
        "DB_filter_step_4.to_csv(\"DB_filter_step_4_SySI_data.csv\", index=False)"
      ],
      "metadata": {
        "id": "igpJ2EbzAFuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5° - CIFOD Filter**"
      ],
      "metadata": {
        "id": "_6shgha0ASFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## URSSA module 1: Importing and pre-processing data"
      ],
      "metadata": {
        "id": "eNcXebwYA-qR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages required\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import interpolate\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import MDS\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from scipy.interpolate import interp1d\n",
        "from plotnine import ggplot"
      ],
      "metadata": {
        "id": "cVSoMaaKBBaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the working directory\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/Step_4_SySI_data\")\n",
        "\n",
        "# Importing data (soil attributes + spectra)\n",
        "Data_all = pd.read_csv(\"DB_filter_step_4_SySI_data.csv\")"
      ],
      "metadata": {
        "id": "8s1W9xFKBL1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Data_all)"
      ],
      "metadata": {
        "id": "k2HUVenuShJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copia o DataFrame Data_all para Data\n",
        "Data = Data_all\n",
        "\n",
        "# Especifica o intervalo das colunas espectrais\n",
        "col_index_min = 15  # Índice da primeira coluna espectral\n",
        "col_index_max = 24  # Índice da última coluna espectral\n",
        "\n",
        "# Seleciona as colunas espectrais\n",
        "Spectra_original = Data.iloc[:, col_index_min:col_index_max+1]  # Espectros do solo (original)\n",
        "\n",
        "# Seleciona a coluna ID_Unico\n",
        "ID_Unico = Data['ID_Unico']\n",
        "\n",
        "# Cria o DataFrame dataset_model contendo as colunas espectrais e a coluna ID_Unico\n",
        "dataset_model = Spectra_original.copy()\n",
        "\n",
        "# Insere a coluna ID_Unico como a primeira coluna\n",
        "dataset_model.insert(0, 'ID_Unico', ID_Unico)\n",
        "\n",
        "# Plotting of the first ten original spectra\n",
        "plt.figure()\n",
        "plt.plot(Spectra_original.columns, Spectra_original.iloc[:10].T.values)  # Use column names as is (without int conversion)\n",
        "plt.title('Original Spectra')\n",
        "plt.xlabel('Wavelength (nm)')\n",
        "plt.ylabel('Reflectance Factor')\n",
        "plt.xticks(rotation=90)  # Rotate x labels if needed\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gktjX7aF3QWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset_model)"
      ],
      "metadata": {
        "id": "0RCvf3lLBewf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## URSSA module 2: Unsupervised spectral clustering of samples\n"
      ],
      "metadata": {
        "id": "R3_x2linBwZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The script can be executed in two ways: hosted on a personal machine or through the Google API."
      ],
      "metadata": {
        "id": "RyiEYXAoCN1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***API Google***"
      ],
      "metadata": {
        "id": "oN9Nql3DCcXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rpy2.robjects as robjects\n",
        "from rpy2.robjects.packages import importr\n",
        "\n",
        "utils = importr('utils')\n",
        "utils.install_packages('randomUniformForest')\n",
        "utils.install_packages('magrittr')\n",
        "utils.install_packages('dplyr')\n",
        "utils.install_packages('tidyverse')"
      ],
      "metadata": {
        "id": "UEEWPu50B0no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### MODULE 2: Unsupervised spectral clustering of samples ####\n",
        "\n",
        "## 1) Unsupervised randomUniformForest classification,\n",
        "## 2) Calculating spectral proximity from URF,\n",
        "## 3) Multidimensional Scaling (MDS),\n",
        "## 4) Clustering samples (K-means)\n",
        "\n",
        "\n",
        "import rpy2.robjects as robjects\n",
        "from rpy2.robjects.packages import importr\n",
        "\n",
        "\n",
        "# Import the R packages\n",
        "randomUniformForest = importr('randomUniformForest')\n",
        "magrittr = importr('magrittr')\n",
        "dplyr = importr('dplyr')\n",
        "tidyverse = importr('tidyverse')\n",
        "\n",
        "\n",
        "dataset_model.to_csv(\"dataset_model.csv\", index=True)\n",
        "\n",
        "# Define the R script\n",
        "r_script = \"\"\"\n",
        "\n",
        "setwd(\"/content\")\n",
        "dataset_model <- read.csv(\"dataset_model.csv\", header = TRUE, sep = \",\", dec = \".\", na.strings = \"NA\", check.names = FALSE)\n",
        "dataset_model <- dataset_model %>% remove_rownames %>% column_to_rownames(var=\"ID_Unico\")\n",
        "\n",
        "wavelength_min <- \"350\"\n",
        "wavelength_max <- \"2500\"\n",
        "\n",
        "results <- unsupervised.randomUniformForest(object = dataset_model %>% select(wavelength_min:wavelength_max), # select spectra\n",
        "                                              baseModel = \"proximity\", # c(\"proximity\", \"proximityThenDistance\", \"importanceThenDistance\")\n",
        "                                              endModel = \"MDSkMeans\", # c(\"MDSkMeans\", \"MDShClust\", \"MDS\", \"SpectralkMeans\")\n",
        "                                              endModelMetric = NULL,\n",
        "                                              samplingMethod = \"with bootstrap\", # c(\"uniform univariate sampling\",\"uniform multivariate sampling\", \"with bootstrap\")\n",
        "                                              MDSmetric = \"metricMDS\", # c(\"metricMDS\", \"nonMetricMDS\")\n",
        "                                              proximityMatrix = NULL,\n",
        "                                              sparseProximities = FALSE,\n",
        "                                              outliersFilter = FALSE,\n",
        "                                              Xtest = NULL,\n",
        "                                              predObject = NULL,\n",
        "                                              metricDimension = 2,\n",
        "                                              coordinates = c(1,2),\n",
        "                                              bootstrapReplicates = 100,\n",
        "                                              clusters = NULL,\n",
        "                                              maxIters = NULL,\n",
        "                                              importanceObject = NULL,\n",
        "                                              maxInteractions = 2,\n",
        "                                              reduceClusters = FALSE,\n",
        "                                              maxClusters = 10,\n",
        "                                              mapAndReduce = FALSE,\n",
        "                                              OOB = FALSE,\n",
        "                                              subset = NULL,\n",
        "                                              seed = 2014,\n",
        "                                              uthreads = \"auto\")\n",
        "\n",
        "results_final_R <- data.frame(\"ID_Unico\" = rownames(dataset_model),\n",
        "                              \"cluster\" = results$unsupervisedModel$cluster,\n",
        "                              dataset_model)\n",
        "\n",
        "write.table(results_final_R, \"results_final_R.csv\", sep = \",\", dec = \".\",row.names = FALSE)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Execute the R script\n",
        "robjects.r(r_script)\n"
      ],
      "metadata": {
        "id": "Qvp29D4NDF34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Personal machine - local execution environment:***\n",
        "  Attention, you must indicate the directory path where R is installed. The \"randomUniformForest\" library requires version 4.3.2. And within the script in R language it is necessary to indicate the path to the directory where the libraries are installed."
      ],
      "metadata": {
        "id": "Dgecr7YTDKmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### MODULE 2: Unsupervised spectral clustering of samples ####\n",
        "\n",
        "## 1) Unsupervised randomUniformForest classification,\n",
        "## 2) Calculating spectral proximity from URF,\n",
        "## 3) Multidimensional Scaling (MDS),\n",
        "## 4) Clustering samples (K-means)\n",
        "\n",
        "############################################################################################\n",
        "#  enter the directory of the R language installed on the machine\n",
        "os.environ['R_HOME'] = 'C:/Program Files/R/R-4.3.2'\n",
        "############################################################################################\n",
        "\n",
        "import rpy2.robjects as robjects\n",
        "from rpy2.robjects.packages import importr\n",
        "\n",
        "# Saving Dataframes to CSV\n",
        "\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/Step_5_SySI_data\")\n",
        "dataset_model.to_csv(\"dataset_model.csv\", index=True)\n",
        "\n",
        "# Define the R script\n",
        "r_script = \"\"\"\n",
        "\n",
        "############################################################################################\n",
        "## R: Directory path to the required hsdar package libraries##                             #\n",
        "#                                                                                          #\n",
        " .libPaths(\"G:/OneDrive/Documentos/Doutorado/protocol_database/libraires\")                 #\n",
        "#                                                                                          #\n",
        "############################################################################################\n",
        "\n",
        "# Load the doParallel library\n",
        "library(doParallel)\n",
        "\n",
        "# Configure parallel cluster (can use desired number of cores)\n",
        "cl <- makeCluster(16)\n",
        "registerDoParallel(cl)\n",
        "\n",
        "library(dplyr)\n",
        "library(tidyverse)\n",
        "library(randomUniformForest)\n",
        "\n",
        "############################################################################################\n",
        "\n",
        "setwd(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/Step_5_SySI_data\")\n",
        "\n",
        "############################################################################################\n",
        "\n",
        "dataset_model <- read.csv(\"dataset_model.csv\", header = TRUE, sep = \",\", dec = \".\", na.strings = \"NA\", check.names = FALSE)\n",
        "dataset_model <- dataset_model %>% remove_rownames %>% column_to_rownames(var=\"ID_Unico\")\n",
        "\n",
        "wavelength_min <- \"Blue_SySI_Sentinel\"\n",
        "wavelength_max <- \"SWIR_2_10m_SySI_Sentinel\"\n",
        "\n",
        "results <- unsupervised.randomUniformForest(object = dataset_model %>% select(wavelength_min:wavelength_max), # select spectra\n",
        "                                              baseModel = \"proximity\", # c(\"proximity\", \"proximityThenDistance\", \"importanceThenDistance\")\n",
        "                                              endModel = \"MDSkMeans\", # c(\"MDSkMeans\", \"MDShClust\", \"MDS\", \"SpectralkMeans\")\n",
        "                                              endModelMetric = NULL,\n",
        "                                              samplingMethod = \"uniform multivariate sampling\", # c(\"uniform univariate sampling\",\"uniform multivariate sampling\", \"with bootstrap\")\n",
        "                                              MDSmetric = \"metricMDS\", # c(\"metricMDS\", \"nonMetricMDS\")\n",
        "                                              proximityMatrix = NULL,\n",
        "                                              sparseProximities = FALSE,\n",
        "                                              outliersFilter = FALSE,\n",
        "                                              Xtest = NULL,\n",
        "                                              predObject = NULL,\n",
        "                                              metricDimension = 2,\n",
        "                                              coordinates = c(1,2),\n",
        "                                              bootstrapReplicates = 100,\n",
        "                                              clusters = NULL,\n",
        "                                              maxIters = 300,\n",
        "                                              importanceObject = NULL,\n",
        "                                              maxInteractions = 5,\n",
        "                                              reduceClusters = FALSE,\n",
        "                                              maxClusters = 7,\n",
        "                                              mapAndReduce = FALSE,\n",
        "                                              OOB = FALSE,\n",
        "                                              subset = NULL,\n",
        "                                              seed = 2014,\n",
        "                                              uthreads = \"auto\")\n",
        "\n",
        "results_final_R <- data.frame(\"ID_Unico\" = rownames(dataset_model),\n",
        "                              \"cluster\" = results$unsupervisedModel$cluster,\n",
        "                              dataset_model)\n",
        "\n",
        "write.table(results_final_R, \"results_final_R.csv\", sep = \",\", dec = \".\",row.names = FALSE)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Execute the R script\n",
        "robjects.r(r_script)"
      ],
      "metadata": {
        "id": "J5AnkCkbDX2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/Step_5_SySI_data\")\n",
        "results_final = pd.read_csv(\"results_final_R.csv\")\n",
        "\n",
        "print(results_final)"
      ],
      "metadata": {
        "id": "r-QvsxQK4A0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### manipulating results in dataframes ###\n",
        "\n",
        "clusters = results_final\n",
        "\n",
        "# Selecting only the 'ID_Unico' and 'cluster' columns from the clusters DataFrame\n",
        "clusters = clusters[['ID_Unico', 'cluster']]\n",
        "\n",
        "# Performing a merge between 'clusters' and 'Data_all' using 'ID_Unico' as the key\n",
        "# This will join both DataFrames, keeping only the rows with matching 'ID_Unico'\n",
        "Soil_data_clustered = pd.merge(clusters, Data_all, on='ID_Unico', how='inner')\n",
        "\n",
        "# Saving the merged DataFrame 'Soil_data_clustered' to a CSV file\n",
        "# The file is saved with a comma as the separator and a dot for decimals\n",
        "Soil_data_clustered.to_csv('Soil_data_clustered.csv', sep=',', decimal='.', index=False)"
      ],
      "metadata": {
        "id": "2jMQa2RfRl3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Identification of soil attribute outliers by Isolation Forest in each cluster"
      ],
      "metadata": {
        "id": "KiIn5_4uED20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages required\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from numpy import where\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from plotnine import *"
      ],
      "metadata": {
        "id": "_8PiVdECELoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/Step_5_SySI_data\")\n",
        "Soil_data_clustered = pd.read_csv(\"Soil_data_clustered.csv\")\n",
        "\n",
        "print(Soil_data_clustered)"
      ],
      "metadata": {
        "id": "DteA1rWfEfYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create an empty DataFrame called results\n",
        "results = pd.DataFrame()\n",
        "\n",
        "# Loop through clusters from 1 to 10\n",
        "for i in range(1, 11):\n",
        "    # Filter the DataFrame to select only the records where the \"cluster\" column is equal to i\n",
        "    data_estratificado = Soil_data_clustered[Soil_data_clustered['cluster'] == i]\n",
        "\n",
        "    # Find the columns starting with \"Blue_SySI_Sentinel\" and ending with \"SWIR_2_10m_SySI_Sentinel\"\n",
        "    colunas_selecionadas = data_estratificado.loc[:, 'Blue_SySI_Sentinel':'SWIR_2_10m_SySI_Sentinel']\n",
        "\n",
        "    # Calculate the mean and standard deviation of the selected columns\n",
        "    medias = colunas_selecionadas.mean(numeric_only=True)\n",
        "    desvio_padrao = colunas_selecionadas.std(numeric_only=True)\n",
        "\n",
        "    # Create a temporary DataFrame with the results for the current cluster\n",
        "    results_cluster = pd.DataFrame({\n",
        "        'cluster': [i] * len(medias),\n",
        "        'wavelength': medias.index,  # Keep the column names as strings\n",
        "        'average_reflectance': medias.values,\n",
        "        'sd': desvio_padrao.values\n",
        "    })\n",
        "\n",
        "    # Add the results of the current cluster to the main DataFrame\n",
        "    results = pd.concat([results, results_cluster], ignore_index=True)\n",
        "\n",
        "# Delete rows where 'average_reflectance' is NaN (null)\n",
        "results = results.dropna(subset=['average_reflectance'])\n",
        "\n",
        "# Dicionário para mapear nomes de colunas para rótulos mais curtos\n",
        "label_map = {\n",
        "    'Blue_SySI_Sentinel': 'Blue (458-523 nm)',\n",
        "    'Green_SySI_Sentinel': 'Green (543-578 nm)',\n",
        "    'Red_SySI_Sentinel': 'Red (650-680 nm)',\n",
        "    'Red_Edge_1_10m_SySI_Sentinel': 'Red Edge 1 (698-713 nm)',\n",
        "    'Red_Edge_2_10m_SySI_Sentinel': 'Red Edge 2 (733-748 nm)',\n",
        "    'Red_Edge_3_10m_SySI_Sentinel': 'Red Edge 3 (773-793 nm)',\n",
        "    'NIR_SySI_Sentinel': 'NIR (785-899 nm)',\n",
        "    'Red_Edge_4_10m_SySI_Sentinel': 'Red Edge 4 (855-875 nm)',\n",
        "    'SWIR_1_10m_SySI_Sentinel': 'SWIR 1 (1565-1655 nm)',\n",
        "    'SWIR_2_10m_SySI_Sentinel': 'SWIR 2 (2100-2280 nm)'\n",
        "}\n",
        "\n",
        "# Atualiza os rótulos das colunas no DataFrame de resultados\n",
        "results['wavelength'] = results['wavelength'].map(label_map).fillna(results['wavelength'])\n",
        "\n",
        "# Create the color palette based on the number of clusters\n",
        "palette = [\"#E41A1C\", \"#377EB8\", \"#4DAF4A\", \"#984EA3\",\"#FF760A\",\n",
        "           \"#FFD700\",\"#0AB2D1\", \"#F781BF\", \"#999999\", \"#66C2A5\", \"#8C564B\",\n",
        "           \"#1F78B4\", \"#B2DF8A\", \"#FB9A99\", \"#FDBF6F\", \"#CAB2D6\", \"#6A3D9A\"]\n",
        "\n",
        "# Desired width and height of the plot\n",
        "plot_width = 14\n",
        "plot_height = 7\n",
        "\n",
        "spectral_behavior = (ggplot(results, aes(x='wavelength',\n",
        "                                         y='average_reflectance',\n",
        "                                         ymin='average_reflectance - sd',\n",
        "                                         ymax='average_reflectance + sd',\n",
        "                                         group='cluster')) +\n",
        "\n",
        "  geom_line(aes(color='factor(cluster)'), size=0.85) +\n",
        "  geom_ribbon(aes(fill='factor(cluster)'), alpha=0.2) +\n",
        "  scale_y_continuous(limits=[0, 0.7]) +\n",
        "  scale_color_manual(values=palette) +  # Manually set the colors\n",
        "  scale_fill_manual(values=palette) +  # Manually set the fill colors\n",
        "  labs(title='Spectral behavior of clusters',\n",
        "       x='Wavelength (nm)',\n",
        "       y='Reflectance factor',\n",
        "       color='Cluster',   # Name the color legend\n",
        "       fill='Cluster') +  # Name the fill legend\n",
        "  theme_minimal() +\n",
        "  theme(figure_size=(plot_width, plot_height),\n",
        "        plot_title=element_text(size=15, face=\"bold\"),\n",
        "        axis_title_x=element_text(size=12, face=\"bold\"),\n",
        "        axis_title_y=element_text(size=12, face=\"bold\"),\n",
        "        axis_text_x=element_text(size=10, angle=45, hjust=1),  # Rotate x-axis labels\n",
        "        legend_text=element_text(size=10),\n",
        "        legend_title=element_text(size=12, face=\"bold\"),\n",
        "        plot_background=element_rect(fill=\"white\")))\n",
        "\n",
        "print(spectral_behavior)\n",
        "\n",
        "dpi_value = 1200\n",
        "\n",
        "# Setting the working directory\n",
        "output_path = \"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/Step_5_SySI_data/SySI_spectral_behavior_of_clusters.png\"\n",
        "spectral_behavior.save(output_path, width=plot_width, height=plot_height, dpi=dpi_value)"
      ],
      "metadata": {
        "id": "mODtL_MOT-Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation\n",
        "Data = Soil_data_clustered.dropna(subset=['C_gkg'])\n",
        "Data = Data.set_index('ID_Unico')\n",
        "\n",
        "# Select the cluster column and the variable of interest\n",
        "Observation = Data[[\"cluster\", \"C_gkg\"]]\n",
        "\n",
        "# Filter null data in the \"Clay_gkg\" column\n",
        "Observation = Observation[Observation['C_gkg'].notnull()]\n",
        "\n",
        "# Specify the range of spectral columns\n",
        "col_index_min = 15  # Index of the first spectral column\n",
        "col_index_max = 24 # Index of the last spectral column\n",
        "\n",
        "# Select the spectral columns\n",
        "Spectra_original = Data.iloc[:, col_index_min:col_index_max+1]\n",
        "\n",
        "# Merge DataFrames to have all the information in one place\n",
        "Observation_final = pd.merge(Observation, Spectra_original, left_index=True, right_index=True, how='left')\n",
        "\n",
        "print(Observation_final)"
      ],
      "metadata": {
        "id": "g3i_Yx4xFMe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Observation_final)"
      ],
      "metadata": {
        "id": "gCbL5Nujc9-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############ Isolation Forest ############\n",
        "\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html\n",
        "\n",
        "# Carregar o dataset\n",
        "data = Observation_final\n",
        "\n",
        "# Remover a coluna 'cluster' para manter apenas as features espectrais\n",
        "features = data.drop(columns=['cluster'])\n",
        "\n",
        "# Inicializar dicionários para armazenar os resultados\n",
        "anomalies_dict = {}\n",
        "plot_data_dict = {}\n",
        "\n",
        "# Iterar sobre cada cluster único\n",
        "for idx, cluster_label in enumerate(sorted(data['cluster'].unique())):\n",
        "\n",
        "    # Filtrar dados para o cluster atual\n",
        "    cluster_data = data[data['cluster'] == cluster_label]\n",
        "\n",
        "    # Remover linhas com valores ausentes e manter apenas as features espectrais\n",
        "    data_filtered = cluster_data[features.columns].dropna()\n",
        "\n",
        "    # Garantir que os nomes das colunas sejam strings\n",
        "    data_filtered.columns = data_filtered.columns.astype(str)\n",
        "\n",
        "    # Normalizar os dados usando StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    data_filtered_scaled = scaler.fit_transform(data_filtered)\n",
        "\n",
        "    # Aplicar Isolation Forest para detectar anomalias\n",
        "    iforest = IsolationForest(n_estimators=100, contamination=0.02, random_state=42)\n",
        "    predictions = iforest.fit_predict(data_filtered_scaled)\n",
        "\n",
        "    # Obter o score de anomalia\n",
        "    anomaly_scores = iforest.decision_function(data_filtered_scaled)\n",
        "    data_filtered['anomaly_score'] = anomaly_scores\n",
        "\n",
        "    # Identificar os índices dos outliers (anomalias)\n",
        "    anom_index = where(predictions == -1)\n",
        "    anomalies = data_filtered.iloc[anom_index]\n",
        "    anomalies['cluster'] = cluster_label  # Adicionar a coluna do cluster\n",
        "\n",
        "    # Armazenar as anomalias no dicionário\n",
        "    anomalies_dict[cluster_label] = anomalies\n",
        "\n",
        "    # Aplicar PCA para reduzir as features espectrais para 2 componentes principais\n",
        "    spectral_features = features.columns[1:]  # Excluir 'Clay_gkg'\n",
        "    pca = PCA(n_components=2)\n",
        "    data_pca = pca.fit_transform(data_filtered[spectral_features])\n",
        "\n",
        "    # Criar um DataFrame com PCs e o atributo 'Clay_gkg'\n",
        "    data_3d = pd.DataFrame(data_pca, columns=['PC1', 'PC2'])\n",
        "    data_3d['C_gkg'] = data_filtered['C_gkg'].values\n",
        "\n",
        "    # Separar os dados entre outliers e não-outliers\n",
        "    outliers_3d = data_3d.iloc[anom_index]\n",
        "    non_outliers_3d = data_3d.drop(index=anom_index[0])\n",
        "\n",
        "    # Armazenar os dados para os plots no dicionário\n",
        "    plot_data_dict[cluster_label] = {\n",
        "        'data_3d': data_3d,\n",
        "        'outliers_3d': outliers_3d,\n",
        "        'non_outliers_3d': non_outliers_3d,\n",
        "        'pca': pca\n",
        "    }\n"
      ],
      "metadata": {
        "id": "AdOsavYk0UYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 3D plot Visualization ###\n",
        "\n",
        "# Define the color palette for the clusters\n",
        "palette = [\"#E41A1C\", \"#377EB8\", \"#4DAF4A\", \"#984EA3\",\n",
        "           \"#FF760A\",\"#FFD700\",\"#0AB2D1\"]\n",
        "\n",
        "# Output directory for the plots\n",
        "output_dir = \"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/Step_5_SySI_data/Organic_Carbon\"\n",
        "\n",
        "# Iterate over each cluster and generate plots\n",
        "for idx, (cluster_label, results) in enumerate(plot_data_dict.items()):\n",
        "    data_3d = results['data_3d']\n",
        "    outliers_3d = results['outliers_3d']\n",
        "    non_outliers_3d = results['non_outliers_3d']\n",
        "    pca = results['pca']\n",
        "\n",
        "    # Create 3D plot\n",
        "    fig = plt.figure(figsize=(18, 16))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # Plot outliers\n",
        "    ax.scatter(outliers_3d['PC1'], outliers_3d['PC2'], outliers_3d['C_gkg'],\n",
        "               c='red', marker='x', s=110, label=\"Outliers\")\n",
        "\n",
        "    # Plot non-outliers with cluster-specific color\n",
        "    cluster_color = palette[idx % len(palette)]\n",
        "    ax.scatter(non_outliers_3d['PC1'], non_outliers_3d['PC2'], non_outliers_3d['C_gkg'],\n",
        "               c=cluster_color, marker='o', alpha=1, edgecolors='black', s=60, label=\"Non-outliers\")\n",
        "\n",
        "\n",
        "    # Label axes with explained variance of PCs and Clay attribute, and increase font size\n",
        "    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0] * 100:.2f}%)', fontweight='bold', fontsize=35, labelpad=37)\n",
        "    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1] * 100:.2f}%)', fontweight='bold', fontsize=35, labelpad=45)\n",
        "    ax.set_zlabel('C gkg', fontweight='bold', fontsize=35, labelpad=54)\n",
        "\n",
        "    # Increase the font size of the tick labels\n",
        "    ax.tick_params(axis='both', which='major', labelsize=33, pad=9)  # Adjusts x and y axis\n",
        "    ax.tick_params(axis='z', which='major', labelsize=33, pad=24)     # Adjusts z axis\n",
        "\n",
        "    # Set z-axis limits from 0 to 900\n",
        "    ax.set_zlim(0, 250)\n",
        "\n",
        "    # Adjust layout to prevent label cutoff\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Add legend and title\n",
        "    ax.legend(fontsize=12)\n",
        "    plt.title(f'3D PCA plot with Outliers for Cluster {cluster_label} (Organic Carbon gkg vs. PC1 & PC2)', fontweight='bold', fontsize=20, y=1.05)\n",
        "\n",
        "    # Save the plot as an image with specified DPI\n",
        "    output_path = f\"{output_dir}cluster_{cluster_label}_outlier_detection.png\"\n",
        "    plt.savefig(output_path, dpi=1000)\n",
        "\n",
        "    # Display the plot\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "2k9F12861Zjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate all DataFrames in the dictionary into a single DataFrame\n",
        "anomalies_combined = pd.concat(anomalies_dict.values())\n",
        "\n",
        "# Reset the index of the combined DataFrame\n",
        "anomalies_combined = anomalies_combined.reset_index()\n",
        "\n",
        "print(anomalies_combined)"
      ],
      "metadata": {
        "id": "L5bEfS_l-ZHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate a list of all sample IDs in anomalies_combined\n",
        "IDs_to_filter = anomalies_combined['ID_Unico'].tolist()\n",
        "\n",
        "# Print the list of sample IDs\n",
        "print(IDs_to_filter)\n",
        "\n",
        "# Summary of the number of samples per cluster\n",
        "summary = anomalies_combined.groupby('cluster').size()\n",
        "\n",
        "# Print the summary by cluster\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "QHypDiBR17_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DB_filter_step_5 = Data.reset_index()\n",
        "\n",
        "# Add a new column \"outliers_step_5_\" filled with False\n",
        "DB_filter_step_5.insert(1, \"outliers_step_5_C\", False)\n",
        "\n",
        "# Update the values to True where ID_Unico is in the list IDs_to_filter\n",
        "DB_filter_step_5.loc[DB_filter_step_5['ID_Unico'].isin(IDs_to_filter), 'outliers_step_5_C'] = True\n",
        "\n",
        "# Create a new DataFrame with only the rows where outliers_step_5_ is True\n",
        "outliers_samples_step_5 = DB_filter_step_5[DB_filter_step_5['outliers_step_5_C'] == True].copy()\n",
        "\n",
        "# Delete the samples where outliers_step_5_ is True\n",
        "DB_filter_step_5 = DB_filter_step_5[DB_filter_step_5['outliers_step_5_C'] == False].copy()\n",
        "\n",
        "print(outliers_samples_step_5)"
      ],
      "metadata": {
        "id": "WijBfdPm19tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the number of samples in the initial dataframe of the attribute that was analyzed,\n",
        "# outliers outliers_samples_step_5, and DB_filter_step_5 dataframe\n",
        "\n",
        "# Print the number of samples in Observation_final\n",
        "print(\"Number of samples in Observation_final:\", Observation_final.shape[0])\n",
        "\n",
        "# Print the number of samples in outliers_samples_step_5_\n",
        "print(\"Number of samples in outliers_samples_step_5_C:\", outliers_samples_step_5.shape[0])\n",
        "\n",
        "# Print the number of samples in DB_filter_step_5_\n",
        "print(\"Number of samples in DB_filter_step_5_C:\", DB_filter_step_5.shape[0])"
      ],
      "metadata": {
        "id": "If4mWN992D5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DB_filter_step_5 and outliers_samples_step_5 DataFrame as a CSV file\n",
        "\n",
        "# Setting the working directory\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/Step_5_SySI_data/Organic_Carbon\")\n",
        "\n",
        "# Save the outliers_samples_3 DataFrame as a CSV file\n",
        "outliers_samples_step_5.to_csv(\"outliers_samples_step_5_C_3.csv\", index=False)\n",
        "\n",
        "# Save the DB_filter_3 DataFrame as a CSV file\n",
        "DB_filter_step_5.to_csv(\"DB_filter_step_5_C_3.csv\", index=False)"
      ],
      "metadata": {
        "id": "T9ngOjBL2Ik7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from plotnine import *\n",
        "\n",
        "# Create the color palette based on the number of clusters\n",
        "palette = [\"#E41A1C\", \"#377EB8\", \"#4DAF4A\", \"#984EA3\", \"#FF760A\",\"#FFD700\",\n",
        "           \"#0AB2D1\", \"#F781BF\", \"#999999\", \"#66C2A5\", \"#8C564B\",\"#1F78B4\",\n",
        "           \"#B2DF8A\", \"#FB9A99\", \"#FDBF6F\", \"#CAB2D6\", \"#6A3D9A\"]\n",
        "\n",
        "# Set the desired plot size\n",
        "plot_width = 10\n",
        "plot_height = 9\n",
        "\n",
        "# Create the plot using plotnine\n",
        "box_plot_1 = (ggplot(Observation_final, aes(x='factor(cluster)', y='C_gkg', fill='factor(cluster)')) +\n",
        "            geom_boxplot(alpha=0.7) +\n",
        "            scale_fill_manual(values=palette) +\n",
        "            scale_y_continuous(limits=[0, 250]) +\n",
        "            labs(title='Box Plot of Organic Carbon gkg for Each Cluster before filter',\n",
        "                 x='Cluster',\n",
        "                 y='C_gkg') +\n",
        "            theme_minimal() +\n",
        "            theme(figure_size=(plot_width, plot_height),\n",
        "                  plot_title=element_text(size=13, face=\"bold\"),\n",
        "                  axis_title_x=element_text(size=10, face=\"bold\"),\n",
        "                  axis_title_y=element_text(size=10, face=\"bold\"),\n",
        "                  plot_background=element_rect(fill=\"white\")))\n",
        "\n",
        "# Display the plot\n",
        "print(box_plot_1)\n",
        "\n",
        "# Create the plot using plotnine\n",
        "box_plot_2 = (ggplot(DB_filter_step_5, aes(x='factor(cluster)', y='C_gkg', fill='factor(cluster)')) +\n",
        "            geom_boxplot(alpha=0.7) +\n",
        "            scale_fill_manual(values=palette) +\n",
        "            scale_y_continuous(limits=[0, 250]) +\n",
        "            labs(title='Box Plot of Organic Carbon gkg for Each Cluster after filter',\n",
        "                 x='Cluster',\n",
        "                 y='C_gkg') +\n",
        "            theme_minimal() +\n",
        "            theme(figure_size=(plot_width, plot_height),\n",
        "                  plot_title=element_text(size=13, face=\"bold\"),\n",
        "                  axis_title_x=element_text(size=10, face=\"bold\"),\n",
        "                  axis_title_y=element_text(size=10, face=\"bold\"),\n",
        "                  plot_background=element_rect(fill=\"white\")))\n",
        "\n",
        "# Display the plot\n",
        "print(box_plot_2)\n",
        "\n",
        "dpi_value = 1000\n",
        "\n",
        "# Setting the working directory\n",
        "output_path = \"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/Step_5_SySI_data/Organic_Carbon/box_plot_1.png\"\n",
        "box_plot_1.save(output_path, width=plot_width, height=plot_height, dpi=dpi_value)\n",
        "\n",
        "output_path = \"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/Step_5_SySI_data/Organic_Carbon/box_plot_2.png\"\n",
        "box_plot_2.save(output_path, width=plot_width, height=plot_height, dpi=dpi_value)"
      ],
      "metadata": {
        "id": "0Gt7YTy3IRoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from plotnine import ggplot, aes, geom_boxplot, scale_fill_manual, scale_y_continuous, labs, theme_minimal, theme, element_text, element_rect\n",
        "\n",
        "# Create the color palette based on the number of clusters\n",
        "palette = [\"#E41A1C\", \"#377EB8\", \"#4DAF4A\", \"#984EA3\", \"#FF7F00\",\n",
        "           \"#FFFF33\", \"#A65628\", \"#F781BF\", \"#999999\", \"#66C2A5\", \"#8C564B\",\n",
        "           \"#1F78B4\", \"#B2DF8A\", \"#FB9A99\", \"#FDBF6F\", \"#CAB2D6\", \"#6A3D9A\"]\n",
        "\n",
        "# Set the desired plot size\n",
        "plot_width = 10\n",
        "plot_height = 9\n",
        "\n",
        "# Create the plot using plotnine for before the filter\n",
        "box_plot_1 = (ggplot(Observation_final, aes(x='factor(cluster)', y='C_gkg', fill='factor(cluster)')) +\n",
        "            geom_boxplot(alpha=0.7) +\n",
        "            scale_fill_manual(values=palette) +\n",
        "            scale_y_continuous(limits=[0, 200]) +\n",
        "            labs(title='Box Plot of C gkg for Each Cluster before filter',\n",
        "                 x='Cluster',\n",
        "                 y='C_gkg') +\n",
        "            theme_minimal() +\n",
        "            theme(figure_size=(plot_width, plot_height),\n",
        "                  plot_title=element_text(size=13, face=\"bold\"),\n",
        "                  axis_title_x=element_text(size=10, face=\"bold\"),\n",
        "                  axis_title_y=element_text(size=10, face=\"bold\"),\n",
        "                  plot_background=element_rect(fill=\"white\")))\n",
        "\n",
        "# Create the plot using plotnine for after the filter\n",
        "box_plot_2 = (ggplot(DB_filter_step_5_C, aes(x='factor(cluster)', y='C_gkg', fill='factor(cluster)')) +\n",
        "            geom_boxplot(alpha=0.7) +\n",
        "            scale_fill_manual(values=palette) +\n",
        "            scale_y_continuous(limits=[0, 200]) +\n",
        "            labs(title='Box Plot of C gkg for Each Cluster after filter',\n",
        "                 x='Cluster',\n",
        "                 y='C_gkg') +\n",
        "            theme_minimal() +\n",
        "            theme(figure_size=(plot_width, plot_height),\n",
        "                  plot_title=element_text(size=13, face=\"bold\"),\n",
        "                  axis_title_x=element_text(size=10, face=\"bold\"),\n",
        "                  axis_title_y=element_text(size=10, face=\"bold\"),\n",
        "                  plot_background=element_rect(fill=\"white\")))\n",
        "\n",
        "# Save the plots with high resolution (1000 dpi)\n",
        "box_plot_1.save(\"box_plot_1.png\", dpi=1000)\n",
        "box_plot_2.save(\"box_plot_2.png\", dpi=1000)\n",
        "\n",
        "# Load the saved images\n",
        "img1 = mpimg.imread(\"box_plot_1.png\")\n",
        "img2 = mpimg.imread(\"box_plot_2.png\")\n",
        "\n",
        "# Set up the side-by-side display\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(plot_width*2, plot_height))\n",
        "\n",
        "# Display the images side by side\n",
        "axes[0].imshow(img1)\n",
        "axes[0].axis('off')  # Remove the axes\n",
        "axes[0].set_title('Box Plot of C gkg for Each Cluster before filter')\n",
        "\n",
        "axes[1].imshow(img2)\n",
        "axes[1].axis('off')  # Remove the axes\n",
        "axes[1].set_title('Box Plot of C gkg for Each Cluster after filter')\n",
        "\n",
        "# Save the combined figure with high resolution (1000 dpi)\n",
        "plt.tight_layout()\n",
        "plt.savefig('combined_box_plots.png', dpi=1000)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lCCbYpfVISBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 6° - ABOPA Filter**"
      ],
      "metadata": {
        "id": "XrMIUcKhMtev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing and manipulating data"
      ],
      "metadata": {
        "id": "NPDBd2rnNDFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages required\n",
        "import os\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "TPm7ZEGPB3Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial Setting up\n",
        "\n",
        "# Set the working directory\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/Step_5_SySI_data/Organic_Carbon\")\n",
        "\n",
        "# Read the CSV file\n",
        "DB_filter_step_5 = pd.read_csv(\"DB_filter_step_5_C_3.csv\")\n",
        "\n",
        "# Assigning to a variable for clarity\n",
        "DB = DB_filter_step_5"
      ],
      "metadata": {
        "id": "zcUytqKZNHIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(DB)"
      ],
      "metadata": {
        "id": "BRYZhh3bNfhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting specific columns where the spectrum begins and ends\n",
        "DB_spectra = DB.iloc[:, 17:27]\n",
        "\n",
        "print(DB_spectra)"
      ],
      "metadata": {
        "id": "zWwCTeo-NiUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Creating a new dataframe with specific columns and removing rows with NA values\n",
        "\n",
        "DB1 = pd.concat([DB.iloc[:, [0, 9]], # select the column number where the dependent variable is\n",
        "\n",
        "                 DB_spectra.iloc[:, list(range(0, 10, 1))]], axis=1)\n",
        "\n",
        "DB1 = DB1.dropna()\n",
        "\n",
        "# Printing the dataframe structure\n",
        "print(DB1.info())\n",
        "\n",
        "print(DB1)\n",
        "\n",
        "# Selecting specific rows for training\n",
        "DB_train = DB1.iloc[:, 2:12]\n",
        "\n",
        "# Selecting a specific column\n",
        "atr_content = DB1['C_gkg']"
      ],
      "metadata": {
        "id": "RH9NGNlCNoMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(DB_train)"
      ],
      "metadata": {
        "id": "Xuw22skiNrWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction via Cubist algorithm"
      ],
      "metadata": {
        "id": "lSOMYTbkNz8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Personal machine - local execution environment:***\n",
        "  Attention, you must indicate the directory path where R is installed. The \"Caret\" library requires version 4.3.2. And within the script in R language it is necessary to indicate the path to the directory where the libraries are installed."
      ],
      "metadata": {
        "id": "358vTKG2OhSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Cubist Algorithm with Cross Validation ###\n",
        "# Imports\n",
        "import os\n",
        "# Se estiver executando o script na máquina pessoal, insira o diretório da linguagem R instalada na máquina\n",
        "os.environ['R_HOME'] = 'C:/Program Files/R/R-4.3.2'\n",
        "import rpy2.robjects as robjects\n",
        "from rpy2.robjects.packages import importr\n",
        "\n",
        "# Set the working directory\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/Step_6_SySI_data/Organic_Carbon\")\n",
        "\n",
        "# Saving Dataframes to CSV\n",
        "DB1.to_csv(\"DB1.csv\", index=False)\n",
        "DB_train.to_csv(\"DB_train.csv\", index=False)\n",
        "atr_content.to_csv(\"atr_content.csv\", index=False)\n",
        "\n",
        "\n",
        "# R Script\n",
        "r_script = \"\"\"\n",
        "\n",
        "############################################################################################\n",
        "## R: Directory path to the required package libraries##                                   #\n",
        "#                                                                                          #\n",
        " .libPaths(\"G:/OneDrive/Documentos/Doutorado/protocol_database/libraires\")                 #\n",
        "#                                                                                          #\n",
        "############################################################################################\n",
        "\n",
        "\n",
        "# Carregue a biblioteca doParallel\n",
        "library(doParallel)\n",
        "\n",
        "# Configure o cluster paralelo (pode usar o número de núcleos desejado)\n",
        "cl <- makeCluster(8)\n",
        "registerDoParallel(cl)\n",
        "\n",
        "library(caret)\n",
        "############################################################################################\n",
        "\n",
        "# Reading CSV files\n",
        "setwd(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/Step_6_SySI_data/Organic_Carbon\")\n",
        "\n",
        "############################################################################################\n",
        "\n",
        "DB1 <- read.csv(\"DB1.csv\", header=TRUE, sep=\",\", dec=\".\", na.strings=\"NA\", check.names=FALSE)\n",
        "DB_train <- read.csv(\"DB_train.csv\", header=TRUE, sep=\",\", dec=\".\", na.strings=\"NA\", check.names=FALSE)\n",
        "atr_content <- read.csv(\"atr_content.csv\", header=TRUE, sep=\",\", dec=\".\", na.strings=\"NA\", check.names=FALSE)\n",
        "\n",
        "# Converting atr_content to numeric\n",
        "atr_content <- as.numeric(unlist(atr_content))\n",
        "\n",
        "# CHOOSE HOW MANY TIMES TO RUN\n",
        "K <- 50\n",
        "\n",
        "# CREATE SPREADSHEETS\n",
        "predictions <- matrix(nrow = length(atr_content), ncol = K)\n",
        "stat_pred_cv <- matrix(nrow = 3, ncol = K)\n",
        "\n",
        "# Get the number of columns in DB_train\n",
        "num_columns <- ncol(DB_train)\n",
        "impcov <- matrix(ncol = K, nrow = num_columns)\n",
        "############################################################################################\n",
        "\n",
        "# Define the column name of the dependent variable\n",
        "depend_var <- \"C_gkg\"\n",
        "\n",
        "############################################################################################\n",
        "\n",
        "# Loop through iterations\n",
        "for (i in 1:K) {\n",
        "\n",
        "  # Check and install required packages\n",
        "  if (!require(parallel)) {install.packages(\"parallel\"); require(parallel)}\n",
        "  detectCores()\n",
        "\n",
        "  if (!require(doParallel)) {install.packages(\"doParallel\"); require(doParallel)}\n",
        "\n",
        "  # Set up parallel processing\n",
        "  cl <- makePSOCKcluster(8) # Choose the number of CPU cores\n",
        "  registerDoParallel(cl)\n",
        "\n",
        "  # Create a new dataframe DB_IDs that includes a column with the indices of the rows\n",
        "  DB_IDs <- cbind(Row_Index = rownames(DB1), DB1)\n",
        "\n",
        "\n",
        "  # Create 10 folds\n",
        "  folds <- createFolds(DB1[[depend_var]], k = 10)\n",
        "\n",
        "  ctrl <- trainControl(method = \"cv\", index = folds)\n",
        "\n",
        "  # Sample rows for training\n",
        "  cal_rows <- sample(nrow(DB_train), nrow(DB_train), replace = TRUE)\n",
        "\n",
        "#########################################################################################\n",
        "# Enter the name of the dependent covariate, matching the name in the data sheet column.\n",
        "\n",
        "    Atr_Cub_Model <- train(x = DB_train[cal_rows, ],\n",
        "                           y = DB1[[depend_var]][cal_rows],\n",
        "                           method = \"cubist\",\n",
        "                           trControl = ctrl)\n",
        "    print(Atr_Cub_Model)\n",
        "\n",
        "#########################################################################################\n",
        "\n",
        "  ###############################################################################################################\n",
        "  # Custom commands to obtain the average RPIQ of the best model\n",
        "  ###############################################################################################################\n",
        "\n",
        "  # Obtain the order of folds in the model output\n",
        "  fold_order <- Atr_Cub_Model[[\"resample\"]][[\"Resample\"]]\n",
        "\n",
        "  # Obtain Rsquared and RMSE values from the model output\n",
        "  rsquared_values <- Atr_Cub_Model[[\"resample\"]][[\"Rsquared\"]]\n",
        "  rmse_values <- Atr_Cub_Model[[\"resample\"]][[\"RMSE\"]]\n",
        "\n",
        "  # Create new vectors of Rsquared and RMSE corresponding to the original fold order\n",
        "  rsquared_ordered <- rep(NA, length(folds))\n",
        "  rmse_ordered <- rep(NA, length(folds))\n",
        "\n",
        "  for (m in 1:length(fold_order)) {\n",
        "    fold_number <- as.numeric(sub(\"Fold\", \"\", fold_order[m]))\n",
        "    rsquared_ordered[fold_number] <- rsquared_values[m]\n",
        "    rmse_ordered[fold_number] <- rmse_values[m]\n",
        "  }\n",
        "\n",
        "  # Add Rsquared and RMSE to the folds list\n",
        "  for (m in 1:length(folds)) {\n",
        "    folds[[m]]$Value_Rsquared <- rsquared_ordered[m]\n",
        "    folds[[m]]$Value_RMSE <- rmse_ordered[m]\n",
        "  }\n",
        "\n",
        "  # Initialize the folds_IDs list\n",
        "  folds_IDs <- list()\n",
        "\n",
        "  # Fill the list with IDs and dependent variable values for each fold\n",
        "  for (m in 1:length(folds)) {\n",
        "    fold_indices <- unlist(folds[[m]])\n",
        "    # Use match to find the corresponding indices in DB_IDs based on Row_Index values\n",
        "    matched_indices <- match(fold_indices, DB_IDs$Row_Index)\n",
        "    fold_data <- DB_IDs[matched_indices, c(\"Row_Index\",\"ID_Unico\", depend_var)]\n",
        "    folds_IDs[[m]] <- fold_data\n",
        "  }\n",
        "\n",
        "  # Initialize lists to store first quartiles and third quartiles\n",
        "  first_quartiles <- vector(\"numeric\", length(folds_IDs))\n",
        "  third_quartiles <- vector(\"numeric\", length(folds_IDs))\n",
        "\n",
        "  # Calculate median and third quartile for each fold\n",
        "  for (m in 1:length(folds_IDs)) {\n",
        "    fold_data <- folds_IDs[[m]][[depend_var]]\n",
        "    first_quartiles[m] <- quantile(fold_data, probs = 0.25, na.rm = TRUE)  # 1st quartile\n",
        "    third_quartiles[m] <- quantile(fold_data, probs = 0.75, na.rm = TRUE)  # 3rd quartile\n",
        "  }\n",
        "\n",
        "  # Add medians and third quartiles to the folds list\n",
        "  for (m in 1:length(folds)) {\n",
        "    folds[[m]]$First_Quartile <- first_quartiles[[m]]\n",
        "    folds[[m]]$Third_Quartile <- third_quartiles[[m]]\n",
        "  }\n",
        "\n",
        "  # Calculate Value_RPIQ for each fold\n",
        "  for (m in 1:length(folds)) {\n",
        "    folds[[m]]$Value_RPIQ <- (folds[[m]]$Third_Quartile - folds[[m]]$First_Quartile) / folds[[m]]$Value_RMSE\n",
        "  }\n",
        "\n",
        "  # Initialize a list to store the RPIQ values\n",
        "  rpiq_values <- vector(\"numeric\", length(folds))\n",
        "\n",
        "  # Store the RPIQ value of each fold in the list\n",
        "  for (m in 1:length(folds)) {\n",
        "    rpiq_values[m] <- folds[[m]]$Value_RPIQ\n",
        "  }\n",
        "\n",
        "  # Print RPIQ values for each fold\n",
        "  for (m in 1:length(folds)) {\n",
        "    cat(\"Fold\", m, \"RPIQ:\", folds[[m]]$Value_RPIQ, \"\\n\")\n",
        "  }\n",
        "\n",
        "  ###############################################################################################################\n",
        "\n",
        "  # Generate a .csv file for each fold\n",
        "  # for (i in 1:length(folds_IDs)) {\n",
        "  #   write.csv(folds_IDs[[i]], file = paste0(\"Fold_\", i, \".csv\"))\n",
        "  # }\n",
        "\n",
        "  # Print the model summary\n",
        "  print(Atr_Cub_Model)\n",
        "\n",
        "  print(Atr_Cub_Model[[\"bestTune\"]])\n",
        "\n",
        "  Atr_Cub_Model[[\"resample\"]][[\"Resample\"]]\n",
        "\n",
        "  Atr_Cub_Model[[\"resample\"]][[\"Rsquared\"]]\n",
        "\n",
        "  Atr_Cub_Model[[\"resample\"]][[\"RMSE\"]]\n",
        "\n",
        "  # Calculate the mean of RPIQ values\n",
        "  average_rpiq <- mean(rpiq_values, na.rm = TRUE)\n",
        "\n",
        "  # Print the mean of RPIQ values\n",
        "  cat(\"The mean of RPIQ values is:\", average_rpiq, \"\\n\")\n",
        "\n",
        "  # Extract and order cross-validation results\n",
        "  CV <- as.data.frame(cbind(Atr_Cub_Model$results$Rsquared, Atr_Cub_Model$results$RMSE))\n",
        "  colnames(CV) <- c(\"R2\",\"RMSE\")\n",
        "  ordenadoCV <- as.data.frame(CV[order(CV$RMSE, decreasing = FALSE), ])[1,]\n",
        "\n",
        "  R2 <- ordenadoCV$R2\n",
        "  RMSE <- ordenadoCV$RMSE\n",
        "  RPIQ <- average_rpiq\n",
        "\n",
        "  restusCV_1 <- rbind(R2, RMSE, RPIQ)\n",
        "\n",
        "  stat_pred_cv[, i] <- restusCV_1\n",
        "\n",
        "  # Covariables importance\n",
        "  ImpCov <- varImp(Atr_Cub_Model)\n",
        "  ImpCoV1 <- as.data.frame(ImpCov$importance)\n",
        "  write.csv(ImpCoV1, \"VAR.csv\")\n",
        "  ImpCoV2 <- read.csv(\"VAR.csv\")\n",
        "  ImpCoV3 <- ImpCoV2[order(ImpCoV2$X, decreasing = FALSE), ]\n",
        "  rownames(ImpCoV3) <- ImpCoV3$X\n",
        "  ImpCoV4 <- ImpCoV3$Overall\n",
        "\n",
        "  # Predictions\n",
        "  pred_Cub <- predict(Atr_Cub_Model, DB_train)\n",
        "\n",
        "  #ID_Unico <- as.numeric(DB1$ID_Unico)\n",
        "  ID_Unico <- as.character(DB1$ID_Unico)  # Ensure ID_Unico is character\n",
        "\n",
        "  Predict <- as.data.frame(cbind(ID_Unico, atr_content, pred_Cub))\n",
        "\n",
        "  pred_Cub2 <- data.frame(pred_Cub)\n",
        "  predictions[, i] <- pred_Cub2[, ncol(pred_Cub2)]\n",
        "  impcov[, i] <- t(ImpCoV4)\n",
        "\n",
        "  # Stop parallel processing\n",
        "  stopCluster(cl)\n",
        "}\n",
        "\n",
        "impcov <- as.data.frame(impcov)\n",
        "impcov$X <- ImpCoV3$X\n",
        "impcov <- impcov[, c(\"X\", setdiff(names(impcov), \"X\"))]\n",
        "\n",
        "############################################################################################\n",
        "\n",
        "setwd(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/Step_6_SySI_data/Organic_Carbon\")\n",
        "write.csv(impcov, \"ImpCoV_DB_filter_step_5_Clay.csv\")\n",
        "write.csv(stat_pred_cv, \"acuracia_DB_filter_step_5_Clay.csv\")\n",
        "\n",
        "############################################################################################\n",
        "\n",
        "### Statistical analysis of predictions ###\n",
        "\n",
        "# Calculate mean prediction for each row\n",
        "mean_pred <- apply(predictions, 1, mean)\n",
        "\n",
        "# Calculate standard deviation for each row\n",
        "sd_pred <- apply(predictions, 1, sd)\n",
        "\n",
        "# Coefficient of Variation (CV) function\n",
        "cv <- function(x) {\n",
        "  sd(x) / mean(x) * 100\n",
        "}\n",
        "\n",
        "# Calculate CV for each row\n",
        "cv_pred <- apply(predictions, 1, cv)\n",
        "\n",
        "# Calculate 90% prediction interval for each row\n",
        "interval_T <- t(apply(predictions, 1,\n",
        "                      function(x) {\n",
        "                        quantile(x,\n",
        "                                 probs = c(0.05, 0.95),\n",
        "                                 na.rm = TRUE)\n",
        "                      }))\n",
        "\n",
        "# Extract lower and upper bounds of the prediction interval\n",
        "Lower_T <- interval_T[, 1]\n",
        "Upper_T <- interval_T[, 2]\n",
        "\n",
        "# Calculate the width of the 90% prediction interval\n",
        "interval_90_PI_pred <- Upper_T - Lower_T\n",
        "\n",
        "# Create a dataframe with all statistical parameters\n",
        "stat_p <- data.frame(cbind(ID_Unico, atr_content, predictions, mean_pred, cv_pred, sd_pred, Lower_T, Upper_T, interval_90_PI_pred))\n",
        "\n",
        "############################################################################################\n",
        "\n",
        "# Write the dataframe to a CSV file\n",
        "setwd(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/Step_6_SySI_data/Organic_Carbon\")\n",
        "write.csv(stat_p, \"stat_p.csv\")\n",
        "\n",
        "############################################################################################\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Executing the R script\n",
        "robjects.r(r_script)\n"
      ],
      "metadata": {
        "id": "1xlbYRsgO7TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cutting line determination"
      ],
      "metadata": {
        "id": "FthgZ2AIQD2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages required\n",
        "import os\n",
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from plotnine import *\n",
        "import scipy.stats as stats\n",
        "from plotnine import ggplot"
      ],
      "metadata": {
        "id": "R3eUv4f2QIEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the working directory\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/Step_6_SySI_data/Organic_Carbon\")\n",
        "\n",
        "# Read the CSV file\n",
        "stat_p = pd.read_csv(\"stat_p.csv\")\n",
        "\n",
        "print(stat_p)"
      ],
      "metadata": {
        "id": "2st4ZktlQTYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Cutting line determination ###\n",
        "\n",
        "# Specify the X and Y columns\n",
        "column_x = stat_p['atr_content']\n",
        "column_y = stat_p['mean_pred']\n",
        "\n",
        "# Create a linear regression model\n",
        "X = sm.add_constant(column_x)  # Add the constant for the intercept term\n",
        "model = sm.OLS(column_y, X).fit()\n",
        "\n",
        "# Calculate predicted values\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Calculate the standard deviation of residuals\n",
        "residuals = model.resid\n",
        "std_residuals = np.std(residuals)\n",
        "\n",
        "# Confidence level (e.g., 98%)\n",
        "confidence_level = 0.99\n",
        "\n",
        "# Quantile of the t distribution\n",
        "quantile_t = stats.t.ppf((1 + confidence_level) / 2, df=len(residuals) - 2)\n",
        "\n",
        "# Calculate upper and lower confidence limits\n",
        "upper_limit_attribute = predictions + quantile_t * (std_residuals * 1.15)\n",
        "lower_limit_attribute = predictions - quantile_t * (std_residuals * 1.15)\n",
        "\n",
        "# Regression coefficients\n",
        "intercept, coef_x = model.params\n",
        "\n",
        "# Standard error of coefficients\n",
        "std_error = model.bse\n",
        "\n",
        "# T-value and p-value of coefficients\n",
        "t_value = model.tvalues\n",
        "p_value = model.pvalues\n",
        "\n",
        "# Coefficient of determination (R²)\n",
        "r_squared_simulated = model.rsquared\n",
        "\n",
        "# Fit a linear regression model for the upper limit\n",
        "model_upper_limit = sm.OLS(upper_limit_attribute, X).fit()\n",
        "\n",
        "# Fit a linear regression model for the lower limit\n",
        "model_lower_limit = sm.OLS(lower_limit_attribute, X).fit()\n",
        "\n",
        "# Coefficients of intercept and slope for the upper limit\n",
        "coef_intercept_upper, coef_slope_upper = model_upper_limit.params\n",
        "\n",
        "# Coefficients of intercept and slope for the lower limit\n",
        "coef_intercept_lower, coef_slope_lower = model_lower_limit.params\n",
        "\n",
        "# Display coefficients\n",
        "print(\"Coefficients for Upper Limit:\")\n",
        "print(\"Intercept (β0):\", round(coef_intercept_upper, 4))\n",
        "print(\"Slope (β1):\", round(coef_slope_upper, 4))\n",
        "\n",
        "print(\"\\nCoefficients for Lower Limit:\")\n",
        "print(\"Intercept (β0):\", round(coef_intercept_lower, 4))\n",
        "print(\"Slope (β1):\", round(coef_slope_lower, 4))"
      ],
      "metadata": {
        "id": "KY6EYwTDQos7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Observed clay vs. predicted clay scatterplot ###\n",
        "\n",
        "# Desired width and height of the plot\n",
        "plot_width = 9\n",
        "plot_height = 6\n",
        "\n",
        "scatter_plot = (\n",
        "    ggplot(stat_p, aes(x='atr_content', y='mean_pred')) +\n",
        "    theme_classic() +\n",
        "    geom_point(shape='o', color= 'black', fill= '#8069C7', alpha=1, size=1.5, stroke=0.2) + # Data points with black contour +  # Data points\n",
        "    geom_smooth(method='lm', se=False, color='#000000', size=1.2) +  # Trend line\n",
        "    geom_line(aes(y=upper_limit_attribute), color='#E41A1C', size=1) +  # Upper confidence interval line\n",
        "    geom_line(aes(y=lower_limit_attribute), color='#E41A1C', size=1) +  # Lower confidence interval line\n",
        "\n",
        "    annotate('text', x=55, y=232, label=\"β upper limit: {:.3f}\".format(round(coef_intercept_upper, 3)),\n",
        "             family=\"Arial\", size=9)+\n",
        "    annotate('text', x=55, y=220, label=\"β lower limit: {:.3f}\".format(round(coef_intercept_lower, 3)),\n",
        "             family=\"Arial\", size=9)+\n",
        "    annotate('text', x=55, y=208, label=\"R² model: {:.3f}\".format(round(r_squared_simulated, 3)),\n",
        "             family=\"Arial\", size=9)+\n",
        "    annotate('text', x=55, y=198, label=\"α: {:.3f}\".format(round(coef_x, 3)),\n",
        "             family=\"Arial\", size=9)+\n",
        "\n",
        "    #labs(x=\"Observed clay content (g kg⁻¹)\", y=\"Predicted clay content (g kg⁻¹)\") +\n",
        "    labs(x=\"Observed C content (g kg$^{-1}$)\", y=\"Predicted C by SySI content (g kg$^{-1}$)\") +\n",
        "    expand_limits(x=0, y=0) +  # Force the plot to show the origin (0,0)\n",
        "    scale_y_continuous(limits=[0, 250]) +\n",
        "    scale_x_continuous(limits=[0, 250]) +\n",
        "    theme(\n",
        "        axis_title_x=element_text(margin={'t': 12}, family=\"Arial\", size=14, face=\"bold\"),\n",
        "        axis_title_y=element_text(margin={'r': 12}, family=\"Arial\", size=14, face=\"bold\"),\n",
        "        axis_text_y=element_text(family=\"Arial\", size=12),\n",
        "        axis_text_x=element_text(family=\"Arial\", size=12)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display the plot\n",
        "print(scatter_plot)\n",
        "\n",
        "dpi_value = 1000\n",
        "\n",
        "# Setting the working directory\n",
        "output_path = \"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/Step_6_SySI_data/Organic_Carbon/scatter_plot_C_predicted_by_SySI.png\"\n",
        "scatter_plot.save(output_path, width=plot_width, height=plot_height, dpi=dpi_value)"
      ],
      "metadata": {
        "id": "rulLkwv6QvGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### After the agreement analysis, filtering the samples in the database ###\n",
        "\n",
        "# Create a subset of samples that are ABOVE the upper_limit_attribute line\n",
        "samples_upper_limit = stat_p[stat_p['mean_pred'] > upper_limit_attribute]\n",
        "\n",
        "# Create a subset of samples that are BELOW the lower_limit_attribute line\n",
        "samples_lower_limit = stat_p[stat_p['mean_pred'] < lower_limit_attribute]\n",
        "\n",
        "# Concatenate the dataframes samples_upper_limit and samples_lower_limit\n",
        "outliers_samples = pd.concat([samples_upper_limit, samples_lower_limit])\n",
        "\n",
        "# Merge the DataFrames based on the ID_Unico column and keep only the columns from the DB DataFrame\n",
        "outliers_samples_6 = pd.merge(outliers_samples[['ID_Unico']], DB, on='ID_Unico', how='inner')\n",
        "\n",
        "IDs_to_filter = outliers_samples['ID_Unico'].tolist()\n",
        "\n",
        "DB_filter_step_6 = DB_filter_step_5\n",
        "\n",
        "# Add a new column \"outliers_step_4\" filled with False\n",
        "DB_filter_step_6.insert(1, \"outliers_step_6\", False)\n",
        "\n",
        "# Update the values to True where ID_Unico is in the list IDs_to_filter\n",
        "DB_filter_step_6.loc[DB_filter_step_6['ID_Unico'].isin(IDs_to_filter), 'outliers_step_6'] = True\n",
        "\n",
        "# Create a new DataFrame with only the rows where outliers_step_4 is True\n",
        "outliers_samples_step_6 = DB_filter_step_6[DB_filter_step_6['outliers_step_6'] == True].copy()\n",
        "\n",
        "# Remove the samples where outliers_step_6 is True\n",
        "DB_filter_step_6 = DB_filter_step_6[DB_filter_step_6['outliers_step_6'] == False].copy()\n",
        "\n",
        "print(outliers_samples_step_6)\n"
      ],
      "metadata": {
        "id": "bG7czURaQ1_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the number of samples in the initial dataframe of the attribute that was analyzed,\n",
        "# outliers outliers_samples_step_6, and DB_filter_step_6 dataframe\n",
        "\n",
        "# Print the number of samples in DB_filter_step_5\n",
        "print(\"Number of samples in DB_filter_step_5:\", DB_filter_step_5.shape[0])\n",
        "\n",
        "# Print the number of samples in outliers_samples_step_6\n",
        "print(\"Number of samples in outliers_samples_step_6:\", outliers_samples_step_6.shape[0])\n",
        "\n",
        "# Print the number of samples in DB_filter_step_6\n",
        "print(\"Number of samples in DB_filter_step_6:\", DB_filter_step_6.shape[0])"
      ],
      "metadata": {
        "id": "my1j0iONRPwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DB_filter_step_6 and outliers_samples_step_6 DataFrame as a CSV file\n",
        "# Setting the working directory\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/SySI_data_filtering/Step_6_SySI_data/Organic_Carbon\")\n",
        "\n",
        "# Save the outliers_samples_6 DataFrame as a CSV file\n",
        "outliers_samples_step_6.to_csv(\"outliers_samples_multispectraldata_step_6_C.csv\", index=False)\n",
        "\n",
        "# Save the DB_filter_6 DataFrame as a CSV file\n",
        "DB_filter_step_6.to_csv(\"DB_filter_multispectraldata_step_6_C.csv\", index=False)"
      ],
      "metadata": {
        "id": "jwDda-nnRTcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analysis of database variability**"
      ],
      "metadata": {
        "id": "wJgxpdrWmndk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Packages required\n",
        "import os\n",
        "import pandas as pd\n",
        "import folium\n",
        "from folium import raster_layers\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import plotnine as p9\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg"
      ],
      "metadata": {
        "id": "KwbeDQ4pmtKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the working directory\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/Step_3\")\n",
        "\n",
        "# Read the CSV file\n",
        "df1 = pd.read_csv(\"outliers_samples_step_3.csv\")\n",
        "\n",
        "os.chdir(\"G:/OneDrive/Documentos/Doutorado/protocol_database/protocol_update/Step_4\")\n",
        "\n",
        "df2=pd.read_csv(\"outliers_samples_step_4.csv\")"
      ],
      "metadata": {
        "id": "fj4K_Fnnm-70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df2)"
      ],
      "metadata": {
        "id": "oLJ_5kiJvJ2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "from folium import raster_layers\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Supondo que 'df1' e 'df2' são os seus DataFrames e 'X' e 'Y' são as colunas de longitude e latitude\n",
        "mapa1 = folium.Map(location=[df1['Y'].mean(), df1['X'].mean()], zoom_start=3, control_scale=True)\n",
        "mapa2 = folium.Map(location=[df2['Y'].mean(), df2['X'].mean()], zoom_start=3, control_scale=True)\n",
        "\n",
        "# Adiciona o mapa de satélite do Google aos dois mapas\n",
        "for mapa in [mapa1, mapa2]:\n",
        "    raster_layers.TileLayer(\n",
        "        tiles = 'https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
        "        attr = 'Google',\n",
        "        name = 'Google Satellite',\n",
        "        overlay = False,\n",
        "        control = True\n",
        "    ).add_to(mapa)\n",
        "\n",
        "# Adiciona marcadores ao mapa1 usando os dados em df1\n",
        "for _, row in df1.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row['Y'], row['X']],\n",
        "        radius=0.8,\n",
        "        color=\"#f44141\",\n",
        "        fill=True,\n",
        "        fill_color=\"#f44141\",\n",
        "        fill_opacity=1\n",
        "    ).add_to(mapa1)\n",
        "\n",
        "# Adiciona marcadores ao mapa2 usando os dados em df2\n",
        "for _, row in df2.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row['Y'], row['X']],\n",
        "        radius=0.8,\n",
        "        color=\"#148fae\",\n",
        "        fill=True,\n",
        "        fill_color=\"#148fae\",\n",
        "        fill_opacity=1\n",
        "    ).add_to(mapa2)\n",
        "\n",
        "# Adiciona controles de camada aos dois mapas\n",
        "folium.LayerControl().add_to(mapa1)\n",
        "folium.LayerControl().add_to(mapa2)\n",
        "\n",
        "# Cria widgets de saída para exibir os mapas\n",
        "out1 = widgets.Output()\n",
        "out2 = widgets.Output()\n",
        "\n",
        "# Exibe os mapas dentro dos widgets de saída\n",
        "with out1:\n",
        "    display(mapa1)\n",
        "with out2:\n",
        "    display(mapa2)\n",
        "\n",
        "# Exibe os widgets de saída lado a lado usando ipywidgets.HBox()\n",
        "widgets.HBox([out1, out2])"
      ],
      "metadata": {
        "id": "Vmpv9JyinJmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria o histograma\n",
        "grafico1 = (p9.ggplot(df1, p9.aes(x='Clay_gkg'))\n",
        "           + p9.geom_histogram(fill='#f44141', binwidth=8)  # Controla a cor das barras\n",
        "           + p9.scale_x_continuous(limits=[0, 900])  # Controla a escala do eixo x\n",
        "           + p9.scale_y_continuous(limits=[0, 50])  # Controla a escala do eixo y\n",
        "          )\n",
        "\n",
        "# Cria o histograma\n",
        "grafico2 = (p9.ggplot(df2, p9.aes(x='Clay_gkg'))\n",
        "           + p9.geom_histogram(fill='#148fae', binwidth=8)  # Controla a cor das barras\n",
        "           + p9.scale_x_continuous(limits=[0, 900])  # Controla a escala do eixo x\n",
        "           + p9.scale_y_continuous(limits=[0, 50])  # Controla a escala do eixo y\n",
        "          )\n",
        "\n",
        "# Salva os gráficos como imagens com alta resolução\n",
        "grafico1.save(\"grafico1.png\", dpi=800)\n",
        "grafico2.save(\"grafico2.png\", dpi=800)\n",
        "\n",
        "# Cria uma nova figura com tamanho maior\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "# Adiciona o primeiro subplot\n",
        "plt.subplot(1,2,1) # (nrows, ncols, index)\n",
        "img = mpimg.imread('grafico1.png')\n",
        "imgplot = plt.imshow(img)\n",
        "plt.axis('off')  # Não mostra os eixos\n",
        "\n",
        "# Adiciona o segundo subplot\n",
        "plt.subplot(1,2,2)\n",
        "img = mpimg.imread('grafico2.png')\n",
        "imgplot = plt.imshow(img)\n",
        "plt.axis('off')  # Não mostra os eixos\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bMJ4BWvt6peZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Clear all variables**"
      ],
      "metadata": {
        "id": "X2yf1A-ZswIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove todas as variáveis do ambiente\n",
        "%reset -f\n",
        "\n",
        "# Remove todas as imports e módulos carregados\n",
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "4YfEjB8Ms78x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}